\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{latex-kit/wacv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{cite}
% This makes floats actually float in the right god-damn place
\usepackage{dblfloatfix}
\usepackage{microtype}
\usepackage{siunitx}
\usepackage{pgf}
\usepackage[percent]{overpic}
\usepackage{color}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

%{\wacvfinalcopy} % *** Uncomment this line for the final submission

\def\wacvPaperID{0000} % Fake paper ID for the quasi-conference
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ifwacvfinal\pagestyle{empty}\fi
\setcounter{page}{1}

% Macros which I find useful:
\renewcommand{\vec}{\mathbf}
\newcommand{\mat}{\mathbf}
\DeclareMathOperator{\nb}{nb}
\DeclareMathOperator{\pa}{pa}
\DeclareMathOperator{\ch}{ch}
% This is just to stop Syntastic from complaining. Grumble grumble
% fixing your tools...
\newcommand{\cyte}[1]{\cite{#1}}

\begin{document}

\title{Articulated Pose Estimation in Videos with Convolutional Neural Networks
and Pose Recombination}

\author{Sam Toyer\\
The Australian National University\\
{\tt\small u5568237@anu.edu.au}}

\maketitle
\ifwacvfinal\thispagestyle{empty}\fi

% Random notes about stuff go here.
%
% Figures which I can include:
% - Diagram showing entire pipeline
% - Correct detections
% - Incorrect detections
% - Illustration of recombination candidate set
% - PCK on PIW for elbows/wrists/shoulders, compared with the same papers which
%   Anoop compares with.
%
% Things I can put in the introduction (not all required; just looking through
% other papers to get ideas):
% - What pose estimation is and why it is important.
% - Challenges in pose estimation
% - Explain different classes of approaches to pose estimation (possibly
%   including their disadvantages). Just need to be careful not to tread on the
%   related work section.
% - Abstractly discuss the approach taken in the paper.
% - Summarise experiments performed and the results of those experiments.
%
% Things I can put in the related work section:
% - Talk about existing approaches and how they compare with one another in
%   terms of results and in terms of the computer vision tools used.
% - Explain why my approach is better (theoretically rather than empirically)
%
% Contents of the intra-frame part:
% - Talk about Yang & Ramanan's articulated model.
% - Discuss the type-based approach to part detection, and its advantages and
%   disadvantages.
% - Mention how the CNN works (esp. the fact that it's a fully convolutional
%   network, and also the way we produce a feature pyramid).
% - Graphical model and inference.
% - Candidate pose generation with NMS at each scale.
% - How do we use distance transforms to make GM inference more efficient? This
%   was something I didn't fully understand, so I might try to write about it in
%   the hope that I come to grok it better.
% - Talk about training.
%
% Contents of the inter-frame part:
% - Just explain how smoothing and recombination works, from optical flow down
%   to producing the final pose.
% - Should also explain whichever extensions I use from Anoop's paper.
% - How do we find our hyperparameters? This is hard, since we don't train
%   hyperparameters; really, we're just fiddling with them until they give us
%   something we want. I could say that I just started with Anoop's parameters,
%   then removed colour tracking and the skin histogram check, which really
%   shows that I have nothing up my sleeves. I could also mention that it's
%   theoretically possible to do a grid search for better parameters (maybe even
%   run such a search and show the results, with the disclaimer that we're
%   training on the test set).
%
% Things to consider putting in the conclusion and future work section (other
% than a summary, obviously):
% - Anoop's suggestion of using a CNN-based regressor to fine-tune joint
%   location estimates. Need to find some prior art on this; did Anoop just make
%   that up when I was talking to him? It seems like a useful insight, so
%   perhaps someone has tried it already.
% - Anoop's other suggestion about using an R-CNN-style approach to CNN forward
%   prop. Not totally convinced about the merits of that (you really only need
%   to identify windows in which there's definitely no human present at all,
%   which isn't what the R-CNN is doing with its semantic segmentation and
%   region joining steps), so I might leave it out.
% - Improve the CNN by using a newer model (e.g. ILSVRC '15 winners) or
%   decreasing the number of output layers from ~10,000 to a few hundred by
%   doing K-means on entire sets of outgoing limbs attached to a single joint.
%   Perhaps there's also a good PCA-based method hiding somewhere in there.
% - Diverse M-best for candidate generation, instead of the crap I'm using at
%   the moment.
% - Using LBP and optical flow to improve pairwise pose estimation.
%
% Remember the Heilmeier Catechism! Jochen said that most of these points should
% be addressed in the discussion:
%
% - What are you trying to do? Articulate your objectives using absolutely no
%   jargon.
% - How is it done today, and what are the limits of current practice?
% - What's new in your approach and why do you think it will be successful?
% - Who cares? If you're successful, what difference will it make?
%
% The Heilmeier Catechism also includes the following questions, although they
% are less relevant for our discussion:
%
% - What are the risks and the payoffs?
% - How much will it cost? How long will it take?
% - What are the midterm and final "exams" to check for success?
%
% The IEEE conference guidelines recommend "Mermin's guide" to typesetting
% mathematics. There are a few interesting rules in it:
% 1) Number everything, whether you intend to refer to it or not. This makes it
%    easier for subsequent readers and people citing your work to find
%    equations, and also makes it easier to describe errata.
% 2) In addition to a number, refer to each equation by some useful name. For
%    example, instead of writing "substituting (7) into (10)", we could write
%    "substituting the temporal cost (7) into the inter-frame cost (10)". This
%    makes it much easier for readers to understand WTF you're talking about.
% 3) End equations with punctuation. Pretend that the equation is prose; if you
%    would normally end that prose with a comma (for example), then you should
%    end the equation with a comma. This rule can be skipped when your equation
%    is embedded in the surrounding text in such a way that, even if it were
%    prose, it would require no trailing punctuation.
% Also, Mermin generally recommends that equations be treated as part of their
% surrounding sentences. If you follow this rule, you won't be including colons
% and the like before equations, nor "introducing" equations ("...as follows:",
% "...the equation:", etc.)
%
% Random note on first-person pronouns: in COMP2550, I used "royal we"
% (referring to yourself as "we"), and it worked okay, so I think I'll do that
% again for this paper. I'm unnaturally attached to scientific "we", so I don't
% want to throw it away :-)

\begin{abstract}
    This paper presents a method for 2D human pose estimation across a sequence
    of video frames. Our approach is comprised of a Convolutional Neural Network
    (CNN) and graphical model for independent generation of pose candidate sets
    in each frame, followed by a recombination step which makes use of optical
    flow and pairwise deformation features to produce a single, consistent
    sequence of poses. Evaluation on the Poses in the Wild data set validates the
    usefulness of inter-frame pose recombination over single-frame pose
    estimation alone, and shows that our method significantly improves upon past
    work in localisation of wrists and elbows.
\end{abstract}

\section{Introduction}
\label{sec:introduction}

The aim of human pose estimation is to take a sequence of static images or video
frames and output a series of two-dimensional ``skeletons'' representing the
locations of the limbs of any humans in those images. Pose estimation is useful
for higher-level computer vision tasks like identifying clothing
items~\cite{liu2012street,liu2012hi,yamaguchi2012parsing}, recognising
actions~\cite{yao2011does}, and even classifying inanimate objects through a
person's interactions with them~\cite{delaitre2012scene}.

Pose estimation is complicated by the wide range of anatomically possible human
poses, the varied appearance of human bodies and clothing, occlusion of body
parts by objects in the scene or by other body parts, background clutter which
resembles body parts, and so on. Taking advantage of the motion information
present in video sequences introduces another layer of complexity to this
already challenging problem.

We deal with this complexity by dividing our video pose estimation pipeline into
two phases. Initially, a pose estimation procedure is applied independently to
each frame in the video sequence. This procedure implicitly scores all possible
poses in each video frame and returns a selection of the highest-scoring poses,
which is pruned through non-maximum suppression to ensure that the returned set
contains a diverse selection of poses. After a candidate pose set has been
generated for each frame, the parts of each pose candidate in each frame are
recombined to reduce each frame's candidate set down to a single high-scoring
pose. This recombination procedure takes into account the optical flow in each
frame, the score of the poses associated with each limb used in recombination,
and the displacements of recombined limbs.

\begin{figure}[t]
\begin{center}
\begin{overpic}[width=0.33\linewidth]{good-shots/frame-1.png}
    \put (90, 80) {1}
\end{overpic}%
\begin{overpic}[width=0.33\linewidth]{good-shots/frame-2.png}
    \put (90, 80) {2}
\end{overpic}%
\begin{overpic}[width=0.33\linewidth]{good-shots/frame-3.png}
    \put (90, 80) {3}
\end{overpic}
% \begin{overpic}[width=0.24\linewidth]{good-shots/frame-4.png}
%     \put (90, 80) {4}
% \end{overpic}
\end{center}
\caption{Excerpt from a video sequence annotated with the detections produced by
our algorithm. Only shoulders, wrists and elbows are shown in this example.}
\label{fig:skeleton}
\end{figure}

Our candidate set generation procedure, described in
Section~\ref{sec:intraframe}, follows the approach of Chen and
Yuille~\cite{chen2014articulated}, which combines features from a deep CNN with
a graphical model to estimate poses in a static image. The CNN yields features
which are much more informative than the Histogram-of-Gradients (HoG) features
used in previous
work~\cite{yang2011articulated,cherian2014mixing,ferrari2008progressive,sapp2013modec},
whilst the graphical model encourages parts to take on
anatomically reasonable relative positions.

As explained in Section~\ref{sec:interframe}, incorporating temporal edges into
graphical model-based pose estimation methods can result in intractable
inference problems because of the loopiness of the resulting graph.
Independently generating a small set of candidate poses in each frame and using
temporal edges to select a candidate from each frame partially alleviates this
problem by reducing the number of poses which must be considered. The
recombination method which we employ, as described by Cherian
\etal~\cite{cherian2014mixing}, improves efficiency even further by allowing
parts to be mixed-and-matched from different poses in the candidate sets in a
top-down fashion, thereby increasing the effective size of the candidate sets
under consideration whilst maintaining low algorithmic complexity.

\section{Related work}

Early attempts at human pose estimation often made use of pictorial structure
models~\cite{fischler1973representation,felzenszwalb2005pictorial}, which
exploit the fact that poses can be decomposed into joints which are likely to
stay within a certain range of distances of one another. Yang and
Ramanan~\cite{yang2011articulated} extended this approach by introducing the
notion of part types to capture the different appearances which a part can have
depending on whether it's open or closed (in the case of hands), whether it's
extended or folded (arms), how much it is rotated, and so on. By treating the
types of each part as latent variables in their graphical model, and augmenting
their deformation terms to take into account the parts on either end of a limb,
Yang and Ramanan were able to obtain a significant increase in accuracy over
traditional pictorial structure models. In this paper, we use a similar approach
for generating pose candidate sets within frames.

Another common approach is to regress the $(x, y)$ coordinates of parts directly
from an image~\cite{toshev2014deeppose,sun2012conditional}, although this can
lead to implausible poses being generated unless some constraints on the
relative positions of limbs or on the overall pose are introduced. Use of
graphical models can increase accuracy by eliminating pose candidates in which
predicted joint positions are individually likely, but collectively implausible
due to the anatomical constraints imposed by a human limbs.

More recently, deep neural networks have been successfully applied to pose
estimation in a variety of ways. One common approach is to attempt to regress
joint coordinates directly; Toshev \etal~\cite{toshev2014deeppose} do this with
a cascade of regressors, where the first regressor outputs an approximate
location for each joint, and subsequent regressors are used to refine those
approximations. Alternative
approaches~\cite{jain2013learning,jain2014modeep,pfister2015flowing} instead map
images to heatmaps for each joint, which has the advantage of being a less
nonlinear mapping than that from images to joint coordinates. The heatmap
approach also makes it straightforward to incorporate graphical models later in
the pose estimation pipeline, since the heatmap values at different locations
can be used as potentials for graphical model
inference~\cite{chen2014articulated,tompson2014joint}, as explained further in
Section~\ref{sec:img-dep}.

The motion information available in videos has previously been exploited by
tracking detected poses throughout a video
sequence~\cite{ji20023d,andriluka2010monocular} or by extending graphical models
for single-frame prediction with temporal links between parts in different
frames~\cite{ferrari2008progressive,cherian2014mixing,sigal2004tracking}. Motion
information has also been incorporated into CNN-based detectors by introducing
motion features like optical flow at the input layers, which can sometimes
increase accuracy over single-frame pose estimation~\cite{jain2014modeep}. As
alluded to in Section~\ref{sec:introduction}, incorporating temporal links into
graphical models tends to result in intractable inference problems, and so past
approaches in this area have generally used approximate methods for inference.
We avoid this problem by generating pose candidate sets independently and taking
the recombination approach described in
Section~\ref{sec:introduction}~\cite{cherian2014mixing}.

\section{Single-frame candidate set generation}
\label{sec:intraframe}

\subsection{Model}
\label{sec:model}

Our method for generating pose candidate sets largely follows that of
\cyte{chen2014articulated}, although we will use this section to describe the
relevant parts of their method in full.

Pose skeletons are represented by a graph $\mathcal G = (\mathcal V, \mathcal
E)$ consisting of a set of vertices (also known as ``parts'' or ``joints'')
$\mathcal V$ and a set of edges (commonly referred to as ``limbs'') $\mathcal E
\subseteq \mathcal V \times \mathcal V$. A complete pose $\vec{p} = (\vec l,
\vec t)$ is represented by a location $\vec{l_u}$ within an image $\mat I$ for
each joint $u \in \mathcal V$, and discrete ``types'' $t_{uv} \in \{1, \ldots,
T_{uv}\}$ and $t_{vu} \in \{1, \ldots, T_{vu}\}$ for each limb $(u, v) \in
\mathcal E$.

Limb types are used to express the orientation and length of limbs. For example,
long forearms running from left to right might be assigned their own discrete
type, as might short, vertical forearms, medium-length diagonal forearms, and so
on. During inference, these types are introduced as latent variables, and
type-dependent limb deformation costs are added to encourage joints to take
anatomically reasonable relative positions. Further evidence for the type of a
limb can be gleaned by inspecting small patches of an image around the endpoints
of that limb. For instance, an image of a shoulder might give clues as to the
direction in which the attached upper arm is pointing; this notion is the
motivating force behind Image-Dependent Pairwise Relations (IDPRs), which are
are discussed at greater length in Section~\ref{sec:img-dep}.

Given an image $\mat I$ and a complete pose $(\vec l, \vec t)$ consisting of a
set of part locations $\vec l$ and a set of limb types $\vec t$, the full score
$C(\mat l, \mat t \mid \mat I)$ of the pose can be decomposed into a sum of
unary costs and pairwise costs, written as
\begin{equation}
\label{eqn:full-cost}
\begin{split}
C(\vec l, \vec t \mid \mat I)
= w_0 + &\sum_{u \in \mathcal V} \phi_u(\vec l_u \mid \mat I)\\
+ &\sum_{(u, v) \in \mathcal E}
    \psi_{uv}(\vec l_u, \vec l_v, t_{uv}, t_{vu} \mid \mat I),
\end{split}
\end{equation}
where $\phi_u$ and $\psi_{uv}$ are described in the next section, and $w_0$ is a
bias term.

\subsection{Image-dependent terms}
\label{sec:img-dep}

The pairwise cost $\psi_{uv}(\vec l_u, \vec l_v, t_{uv}, t_{vu} \mid \mat I)$
can be decomposed into the sum of type-dependent deformation costs and IDPR
terms, as expressed by
\begin{equation}
\begin{split}
\psi_{uv}(\vec l_u, \vec l_v, t_{uv}, t_{vu})
= &\vec w_{uvt_{uv}}^T d(\vec l_v - \vec l_u - \vec r_{uv t_{uv}})\\
+ &\vec w_{vut_{vu}}^T d(\vec l_u - \vec l_v - \vec r_{vu t_{vu}})\\
+ &w_{uv} \mathcal I_{uv}(\vec l_u, t_{uv}, \mat I)\\
+ &w_{vu} \mathcal I_{vu}(\vec l_v, t_{vu}, \mat I),
\end{split}
\end{equation}
where $d(\vec v) = \begin{bmatrix}v_x^2 & v_y^2 & v_x & v_y\end{bmatrix}^T$ is a
deformation feature, and $\mathcal I$ represents an IDPR term, explained below.

Given a $K$-joint skeleton, we can define $p(j = u \mid \mat I(\vec l))$ to be
the probability that the joint contained in the patch $\mat I(\vec l)$ of $\mat
I$ around $\vec l$ is the joint represented by $u \in \{1, \ldots, K\} \cup
\{0\} = \mathcal V \cup \{0\}$, with the special value of $u = 0$ indicating
that no joint is present. If we know that the patch $\mat I(\vec l)$ contains a
joint $u$, and $(u, v) \in \mathcal E$ is a limb, then we can define $p(t_{uv} =
t \mid j = u, \mat I(\vec l))$ to be the probability that the limb between $(u,
v)$ has type $t_{uv} \in \{1, \ldots, T_{uv}\}$. Using this notation, we can
define the IDPR term $\mathcal I$ as
\begin{equation}
\label{eqn:idpr}
\mathcal I_{uv}(\vec l_u, t, \mat I)
= \log p(t_{uv} = t \mid j = u, \mat I(\vec l_u)).
\end{equation}

The inclusion of both $\mathcal I_{uv}$ and $\mathcal I_{vu}$ ensures that the
visual cues given by the parts at either end of a limb can be used to infer the
type of that limb.

The appearance term $\phi_u$ is defined similarly, and gives the log probability
that a small, fixed-size patch of the image centered at $\vec{l}_u$ contains the
joint $u$:
\begin{equation}
\label{eqn:unary}
\phi_u(\vec l_u \mid \mat I)
= w_u \log p(j = u \mid \mat I(\vec l_u)).
\end{equation}

\subsection{Computing unaries and IDPR terms}
\label{sec:cnn}

To compute the unaries defined by (\ref{eqn:unary}) and the IDPR terms defined
by (\ref{eqn:idpr}), we train a CNN to output the joint distribution over part
locations and neighbouring limb types for each patch of a given image $\mat I$,
from which we may obtain the appearance and IDPR terms by marginalisation. If we
let $t_u \in \prod_{(u, v) \in \mathcal E} \{1, \ldots, T_{uv}\}$ denote a
combination of types for all limbs adjacent to a part $u$, then we can write
this distribution as
\begin{equation}
\label{eqn:cnn-output}
p(j = u, t_u = t \mid \mat I(\vec l))
\end{equation}
for any location $\vec l$ in $\mat I$ and any part $u \in \{1, \ldots, K\}$.

If we discard combinations of $u$ and $t$ for which the joint probability is
always zero, then this becomes a discrete distribution over
\begin{equation}
\sum_{u \in \mathcal V} T^{|\{(u, v) : (u, v) \in \mathcal E\}|} + 1
\end{equation}
values, where $\nb(u)$ denotes the neighbours of part $u$ and we assume that
$T_{uv} = T \in \mathbb N$ for all $(u, v) \in \mathcal E$.

Our CNN architecture closely follows that of
AlexNet~\cite{krizhevsky2012imagenet}, and is identical to that of
\cyte{chen2014articulated}. To minimise wasted computation, we convert the final
fully connected layers of the network to $1 \times 4096$ convolutions after the
network has been trained. This allows us to evaluate (\ref{eqn:cnn-output}) over
a set of uniformly spaced patches of a full-resolution image in a single pass,
rather than having to pass patches through the network one at a time.

\subsection{Producing the candidate set}

Having evaluated appearance and IDPR terms for all joints and all locations in
the image, we can now produce a set of high-scoring pose candidates for use in
the recombination procedure (Section~\ref{sec:interframe}). Recall from
Section~\ref{sec:model} that poses are modelled as trees rooted at the head. In
this case, the score of any subtree of the full pose tree rooted at part $u$ in
location $\vec l_u$ is:

\begin{equation}\label{eqn:gm-local-score}
\begin{split}
S_u&(\vec l_u, \mat I) =\\
&\sum_{pa(v) = u} \max_{\vec l_v, t_{uv}, t_{vu}} \left[
\psi_{uv}(\vec l_u, \vec l_v, t_{uv}, t_{vu} \mid \mat I)
+ S(\vec l_v, \mat I)
\right]\\
&+ \phi_u(\vec l_u \mid \mat I).
\end{split}
\end{equation}

At the leaves, this formula becomes $S_v(\vec l_v, \mat I) = \phi_v(\vec l_v
\mid \mat I)$, which is trivial to compute for all locations in the image.
Otherwise, given child scores $S_{v_1}(\vec l_{v_1}, \mat I), \ldots,
S_{v_C}(\vec l_{v_C}, \mat I)$ for the children $\{v : \pa(v) = u\}$ of some
non-leaf part $u$, it is possible to evaluate $S_u(\vec l_u, \mat I)$ for all
locations $\vec l_u$ in linear time using a distance
transform~\cite{felzenszwalb2012distance}. If we have $T_{uv} = T$ for each $(u,
v) \in \mathcal E$, then we must also perform maximisation over $T^2$ limb label
combinations at each joint. Since this maximisation must be performed at a total
of $K$ joints, the overall time complexity of calculating the score
$S_h(\vec{l}_h, \mat I)$ of the root component for all locations $\vec l_h$ is
$O(T^2 N K)$, where $N$ is the number of grid locations.

Given the maximum scores $S_h(\vec l_h, \mat I)$ for a pose rooted at each
possible head location $\vec l_h$, we can produce a set of $M$ pose candidates
by choosing the $M$ highest-scoring head positions and backtracking to find the
remainder of the pose. However, since recombination
benefits from a diverse set of poses, we apply
non-maximum suppression to ensure that our returned candidate pool contains only
poses for which the pairwise intersection-over-union for detected wrists is no
greater than some threshold. This prevents the candidate generation algorithm
from producing a set of poses which each differ by only a few pixels.

\subsection{Learning}
\label{sec:intra-learning}

Training for the single-frame candidate generation model begins with derivation
of the mean limb displacement $\vec r_{uv t_{uv}}$ for each limb $(u, v) \in
\mathcal E$ and each type $t_{uv} \in \{1, \ldots, T\}$ for that limb, where we
have assumed for simplicity that each limb has the same number of types $T$. For
a limb $(u, v)$, this is achieved by calculating the displacement $\vec l_v -
\vec l_u$ associated with each pose in the training set, then running $K$-means
to find $T$ centroids for the calculated displacements.

Having assigned a type to each limb in the training set, an image crop is made
around each joint and labelled with the joint type and the types of all
neighbouring limbs. We also include a set of patches not containing any people,
which are labelled with a special negative label. The produced set of patches
and labels is used to find parameters for the neural network described in
Section~\ref{sec:cnn} using stochastic gradient descent.

Finally, we can learn the bias $w_0$ and the weight sets $\{\vec w_{uv
t_{uv}}\}$, $\{w_{uv}\}$, and $\{w_u\}$ for the pose cost (\ref{eqn:full-cost}).
The cost $C(\vec l_n, \vec t_n \mid \mat I_n)$ associated with the limb
locations $\vec l_n$, image $\mat I_n$ and limb types $\vec t_n$ detected from
the $n$th training sample can be represented as an inner product between a
weight vector $\vec w$ and a feature vector $\mat \Phi_n$ composed of all
appearance, IDPR and deformation terms, with a constant bias concatenated to the
end. We label the $n$th training sample with $y_n = 1$ if the training sample
corresponds to a labelled pose with correctly detected type labels, and $y_n =
-1$ if the training sample corresponds to a negative image with no pose or if
the type labels are not correctly inferred. We can now cast learning of the
graphical model weights as an optimisation problem in which we must find the
weight vector $\vec w^*$ satisfying
\begin{align}
\vec w^* &= \min_{\vec w}\left[ \frac 12 \|\vec w\|^2 + C \sum_n E_n(\vec w)\right],\\
E_n(\vec w) &= \max(0, 1 - y_n \vec w^T \mat \Phi_n).
\end{align}

We calculate $\vec w^*$ efficiently using the dual coordinate descent approach
described in \cyte{yang2011articulated}.

\section{Pose estimation in videos}
\label{sec:interframe}

% \begin{figure*}[t]
% \begin{center}
% \fbox{\rule{0pt}{1in} \rule{0.9\linewidth}{0pt}}
% \end{center}
% \caption{Diagram showing the recombination process. First, a fixed number of
% candidate poses are generated in each frame, sorted descending by score. Next,
% non-maximum suppression is performed on wrists and elbows to increase the
% diversity of the top $N$ candidates. Finally, the top $N$ candidates in each
% frame are fed to the recombination procedure, which greedily recombines limbs in
% a top-down fashion whilst trying to minimise (\ref{eqn:recomb-cost}).}
% \label{fig:recombination}
% \end{figure*}

\subsection{Motivation}

We have already seen in Section~\ref{sec:intraframe} how we can use a graphical
model to do pose estimation within a single frame. Pose estimation in videos is
similar to pose estimation in static images, except that we wish to enforce some
sort of temporal consistency between poses. The obvious approach to this problem
is to apply our existing graphical model to each frame, but to also add some
temporal consistency term $\tau(p_t, p_{t+1}) = \sum_{u \in \mathcal V}
\tau_u(\vec l_{u,t}, \vec l_{u,t+1})$, where $\vec l_{u,t}$ denotes the location
of part $u$ at time $t$. If we had $F$ frames in total, then the full cost would
be
\begin{equation}\label{eqn:hypo-interframe}
C(p_F, \mat I_F) + \sum_{t=1}^{F-1}
\left[C(p_t, \mat I_t) + \tau(p_t, p_{t+1})\right].
\end{equation}

(\ref{eqn:hypo-interframe}) corresponds to a complex, highly loopy graph, which
makes it infeasible to find the $p_1, \ldots, p_F$ which minimises
(\ref{eqn:hypo-interframe}) for any nontrivial choice of $\tau$. One way to
reduce this computational burden is to restrict the set of poses which we
consider to some limited set $\mathcal P_t$ in each frame; if we have exactly
$|\mathcal P|$ candidate poses in each frame, then dynamic programming would
allow us to minimise ($\ref{eqn:hypo-interframe}$) in $O(|\mathcal P|^2 F)$
time. $|\mathcal P|^2$ can still be colossal when a large number of poses are
considered in each frame, which restricts the applicability of this technique to
situations in which $|\mathcal P|$ is small.

\subsection{Recombination}

The method in \cyte{cherian2014mixing} avoids the penalty incurred by large pose
candidate sets by taking a small, diverse set of $|\mathcal P|$ candidate poses
and then considering all possible combinations of joints from each of those
poses. Given $|\mathcal P|$ poses and $K$ joints, this results in an effective
candidate set of ${|\mathcal P|}^K$ poses in each frame, and ensures that the
best joints in the pose candidate set, rather than just the best candidates, are
considered.

Now that we know we can efficiently perform inference on a large effective pose
set, we can introduce temporal smoothing links between each part $u \in \mathcal
V$ in the pose at time $t$ and its counterpart at time $t+1$ using a cost
\begin{equation}\label{eqn:temporal-cost}
\begin{split}
\tau_u&(\vec l_{u,t}, \vec l_{u,t+1}, \mat I_t, \mat I_{t+1})\\
&= \lambda_\tau \|\vec l_{u,t+1}
- \vec l_{u,t}
- f(\vec l_{u,t}, \mat I_t, \mat I_{t+1})\|^2,
\end{split}
\end{equation}
where $f(\vec l_{u,t}, \mat I_t, \mat I_{t+1})$ is the optical flow at location
$\vec l_{u,t}$ between frame $\vec I_t$ and frame $\vec I_{t+1}$.

Additionally, to encourage the connected limbs chosen to make up each frame's
final, recombined pose to be close to one another, we introduce a recombination
cost $\rho_v$ for each pair of limbs $(u, v), (v, w) \in \mathcal E$ (where $u
\neq w$) which share a common part $v$:

\begin{equation}\label{eqn:recomb-strength}
\rho_v(\vec l_v, \vec l_v') = \lambda_\rho \|\vec l_v - \vec l_v'\|^2.
\end{equation}

The complete cost which the recombination process must minimise is therefore
given in (\ref{eqn:recomb-cost}); we have used $\mathcal V_S = \{v : \exists u
\neq w : (u, v) \in \mathcal E \land (v, w) \in \mathcal E\}$ to denote the set
of parts which are shared between two or more limbs, whilst $\vec l_v$ denotes
the location of part $v$ in a recombined pose and $\vec l_v'$ denotes a location
of part a $v$ which was discarded during recombination, but for which the
location of some part $w$ \emph{connected} to $v$ was used.

\begin{equation}\label{eqn:recomb-cost}
\begin{split}
C(p_F, \mat I_F) &+ \sum_{v \in \mathcal V_S} \rho_v(\vec l_{v,F}, \vec
l_{v,F}')\\
+ \sum_{t=1}^{F-1} \biggl[
    &C(p_t, \mat I_t)
    + \sum_{v \in \mathcal V_S} \rho_v(\vec l_{v,t}, \vec l_{v,t}')\\
    &+ \sum_{u \in \mathcal V}
        \tau_u(\vec l_{u,t}, \vec l_{u,t+1}, \mat I_t, \mat I_{t+1})
\biggr]
\end{split}
\end{equation}

In order to make the minimisation of the full temporal cost
(\ref{eqn:recomb-cost}) tractable, limbs are recombined starting at the
head---which is typically the easiest part to detect---then moving on to the
neck, the shoulders, and so on until the full pose has been estimated in all
frames.

Specifically, the algorithm starts by choosing a head position $\vec l_{h,t}$ at
each time $t = 1, \ldots, F$ by choosing a head position from the set $\mathcal
P_t$ in each frame such that the chosen sequence of heads minimises the
following cost, which corresponds to the parts of the full cost
(\ref{eqn:hypo-interframe}) that involve the head or any temporal links between
heads in adjacent frames:
\begin{equation}\label{eqn:head-cost}
\phi_h(\vec l_{h,F}, \mid \mat I_F)
+ \sum_{t=1}^{F-1} \left[
    \phi_h(\vec l_{h,t}, \mid \mat I_t)
    + \tau_h(\vec l_{h,t}, \vec l_{h,t+1})
\right].
\end{equation}

If we have $|\mathcal P|$ candidate poses in each frame, each of which
corresponds to a single head position candidate, then we can use dynamic
programming to perform this minimisation in $O(|\mathcal P|^2 F)$ time.

Position sequences for any subsequent part $u$ can be found in much the same
way, except that we must also include pairwise costs from the single-frame cost
$C$, as well as recombination costs relative to the (previously localised) parent
part, yielding a full cost of
\begin{equation}\label{eqn:subsequent-part-cost}
\begin{split}
C_{uv}(l_{u,F}, &\vec l_{v, F}, \vec t \mid \mat I_F)
+ \rho_u(\vec l_{u,F}, \vec l_{u,F}')\\
+ \sum_{t=1}^{F-1} &\bigl[
    C_{uv}(l_{u,t}, \vec l_{v,t}, \vec t \mid \mat I_F)\\
    &\ + \rho_u(\vec l_{u,t}, \vec l_{u,t}') + \tau_u(\vec l_{u,t}, \vec l_{u,t+1})
\bigr],
\end{split}
\end{equation}
where $v = \pa(u)$ is the parent of $u$ and $C_{uv}(\vec l_u, \vec l_v, \vec t
\mid \mat I)$ are the terms of the single-frame cost (\ref{eqn:full-cost}) which
either involve only part $u$ or involve both $u$ and $v$.

As with the head, finding the appropriate sequence of part locations for each
remaining part can be done in $O(|\mathcal P|^2 F)$ time with dynamic
programming, meaning that the total runtime of the minimisation procedure is
$O(K |\mathcal P|^2 F)$ for a $K$-part skeleton.

\subsection{Approximations and heuristics}

In practice, the unary terms in the head sequence cost (\ref{eqn:head-cost}) and
the cost (\ref{eqn:subsequent-part-cost}) for subsequent parts can be
approximated by the full, single-frame inference score $C(\vec l, \vec t \mid
\mat I)$ for the specific candidate pose being considered. This not only makes
implementation easier, but improves performance due to the fact that the
single-frame inference scores are already computed during candidate set
generation.

In addition to the costs listed above, we have used the ``practical extensions''
of \cyte{cherian2014mixing}, which include additional keypoints along limbs to
constrain motion further, an additional term for wrists which encourages them to
occupy regions of high motion, and a regularisation term which acts to constrain
the absolute difference between part positions across frames.

% NOTE: This has been taken out of the normal page flow so that it appears in the
% right place. This should be corrected before submission.
\begin{figure*}[ht!]
\begin{center}
\begin{tabular}{@{}c@{}c@{}c@{}c@{}c@{}c@{}}
\includegraphics[height=0.14\linewidth]{bad-shots/seq-50-frame-26.png}\,&
\includegraphics[height=0.14\linewidth]{bad-shots/seq-70-frame-30.png}\,&
\includegraphics[height=0.14\linewidth]{bad-shots/seq-62-frame-2.png}\,&
\includegraphics[height=0.14\linewidth]{bad-shots/seq-63-frame-9.png}\,&
\includegraphics[height=0.14\linewidth]{bad-shots/seq-39-frame-14.png}\,&
\includegraphics[height=0.14\linewidth]{bad-shots/seq-73-frame-30.png}\\
(a) & (b) & (c) & (d) & (e) & (f)
\end{tabular}
\end{center}
\caption{Common types of errors encountered during testing on the Poses in the
Wild data set.}
\label{fig:qualitative}
\end{figure*}

\begin{figure*}[ht!]
\begin{center}
\input{plots/accuracy.pgf}
\end{center}
\caption{PCK over the entire Poses in the Wild data set for different body parts.
A part is considered to be correctly detected in a single frame if the distance
between its predicted position and its true position is less than the pixel
distance threshold indicated on the $x$ axis of the graph. Note that the curve
for the default configuration in the shoulder plot is hidden by that of the
static-only configuration.}
\label{fig:quantitative}
\end{figure*}

\section{Experiments}
\label{sec:experiments}

\subsection{Setup}

To evaluate the performance of our model, we tested it on the Poses in the
Wild~\cite{cherian2014mixing} data set, which consists of a series of 16--30
frame sequences extracted from movies. Many sequences in this data set include a
large degree of camera motion, rapidly moving subjects, cluttered backgrounds or
occlusion of body parts.

The single-frame pose estimation model was trained on the Frames Labelled in
Cinema (FLIC) data set~\cite{sapp2013modec}, with negatives drawn from the INRIA
person data set.{\footnotemark} The data set was augmented by rotating images
through a \SI{70}{\degree} range in \SI{5}{\degree} increments, as done in
\cyte{chen2014articulated}.

\footnotetext{\url{http://pascal.inrialpes.fr/data/human/}}

For recombination, we chose a set of hand-tuned parameters which differed for
each pair of parts, depending on the extent of motion of the parts. The precise
set of parameters are available online, along with the rest of the code for
these experiments.\footnote{\url{https://github.com/qxcv/comp2560}} At test
time, 100 candidate poses are used in each frame and NMS is performed at a
threshold of 95\% of the intersection-over-union on each wrist.

We have found it advantageous to perform candidate set generation independently
at several scales. The highest-scoring poses over all scales were then passed to
the recombination stage to produce a final pose sequence.

\subsection{Results}

The mean Percentage Correct Keypoints (PCK) for our algorithm, averaged over all
sequences in Poses in the Wild, is given in Figure~\ref{fig:qualitative}. For
comparison, we have included the results of our algorithm in its previously
described evaluation configuration (``Default config'') and the results of our
algorithm when only one candidate pose is generated for each frame (``Static
only''). For comparison, we have also included the results of Cherian
\etal~\cite{cherian2014mixing} on the same sequence.

% \footnote{The results of Pfister \etal~\cite{pfister2015flowing}---which
% include PCK curves for Poses in the Wild---are also of interest for comparison
% purposes, although they were not available for inclusion in this paper.}

% TODO: Timings
% Timings for the different stages of the pose estimation pipeline during the test
% phase are given in Table~\ref{tab:runtime}. All experiments were performed on a
% using two Intel Xeon E5-2620 processors and an NVIDIA K80 GPU, although the
% parallelism offered by the CPUs is not factored into the runtimes listed in
% Table~\ref{tab:runtime}.
% 
% \begin{table}
% \begin{center}
% \begin{tabular}{|l|c|}
% \hline
% Task & Mean time per frame\\
% \hline\hline
% Optical flow & XXX s\\
% CNN evaluation & XXX s\\
% GM inference & XXX s\\
% Recombination & XXX s\\
% \hline
% \end{tabular}
% \end{center}
% \caption{Runtime cost of different stages of the pipeline. In practise, optical
% flow and candidate set generation can be performed in parallel, which can
% significantly increase performance when parallel computing resources are
% available.}
% \label{tab:runtime}
% \end{table}

\subsection{Discussion and future work}

Figure~\ref{fig:qualitative} shows that our algorithm significantly improves on
the results of \cyte{cherian2014mixing} for elbows and wrists, which are
typically the most difficult body parts to detect. Further, it demonstrates that
recombination yields an appreciable performance increase over independent pose
estimation in each frame. This is particularly true for the wrists, which are
generally the fastest-moving body parts in a video sequence.

Counterintuitively, the approach of Cherian \etal outperforms ours on shoulders
at low thresholds. This could be because of the size of part--type joint
distribution (\ref{eqn:cnn-output}) learnt by the CNN; in the graphical model
used to generate pose candidate sets, the shoulder is attached to the upper arm,
upper torso and base of the neck, so if 13 types were learnt for each limb, then
there would be $13^3 = 2197$ different combinations of adjacent limb types for
each shoulder. Many of these combinations may not be well-represented in the
training set, which would hurt performance at test time.

It may be possible to address this by adopting a part-based type
system~\cite{yang2011articulated} in place of a limb-based one. This would give
greater control over the effective number of part types, which could allow for
the size of the part--type joint distribution to be decreased significantly.
This could also yield an increase in performance, since the CNN evaluation time
for the final softmax layer in the network would decrease, as would the time
taken to copy and marginalise over the hundreds of thousands joint distribution
values produced by applying the fully convolutional network to large images.

% Accuracy could also be boosted by adopting the strategy of Park and
% Ramanan~\cite{park2011n} for generating pose candidate sets. In
% \cyte{park2011n}, Park and Ramaman argue that a large number of high-scoring
% poses are typically not accessible by backtracking from the max marginal table
% for the root node of the model used for pose estimation. Instead, they present
% an algorithm which is able to efficiently j_

Figure~\ref{fig:qualitative} illustrates a number of common failure modes.
Whilst our algorithm is robust to minor occlusions---where a hand lies slightly
outside a frame, for instance---of the kind shown in frames (a) and (d)--(f),
self-occlusion of subjects and partial occlusion of several body parts have
proven more challenging, as in frames (b) and (f). Frames (a) and (c) also show
situations in which limb-like objects have confused the detector. Finally, rapid
limb motion was responsible for a large number of failures, including examples
(d)--(f).

\section{Conclusion}

This paper presented a two-stage approach to pose estimation in videos. In the
first stage, we generated a candidate pose set in each frame independently using
a graphical model incorporating CNN-derived features and type-based limb
deformation costs. In the second stage, we recombined the limbs associated with
each pose in each candidate set to produce a single best sequence of poses using
an approximate top-down approach to avoid the computational intractability
common to loopy graphical models. Evaluation on the challenging Poses in the
Wild data set showed that, for elbows and shoulders, our method significantly
improved on that of \cite{cherian2014mixing}. We closed with a discussion of the
accuracy and failure modes of the algorithm, and suggested that both accuracy
and performance could be improved by giving type labels to joints rather than
limbs.

\ifwacvfinal{%
\noindent\textbf{Acknowledgements}\quad
We would like to thank the authors of \cyte{chen2014articulated} and
\cyte{cherian2014mixing} for making their code publicly available.
}\fi

\ifwacvfinal\else\clearpage\fi
{\small
\bibliographystyle{latex-kit/ieee}
\bibliography{citations}
}
\end{document}
