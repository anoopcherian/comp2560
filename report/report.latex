\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{latex-kit/wacv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{cite}
% This makes floats actually float in the right god-damn place
\usepackage{dblfloatfix}
\usepackage{microtype}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

%{\wacvfinalcopy} % *** Uncomment this line for the final submission

\def\wacvPaperID{0000} % Fake paper ID for the quasi-conference
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ifwacvfinal\pagestyle{empty}\fi
\setcounter{page}{1}

% Macros which I find useful:
\renewcommand{\vec}{\mathbf}
\newcommand{\mat}{\mathbf}
\DeclareMathOperator{\nb}{nb}
\DeclareMathOperator{\pa}{pa}
\DeclareMathOperator{\ch}{ch}
% This is just to stop Syntastic from complaining. Grumble grumble
% fixing your tools...
\newcommand{\cyte}[1]{\cite{#1}}

\begin{document}

%%%%%%%%% TITLE
\title{Pose Estimation in Videos Using Convolutional Neural Networks and
Inter-Frame Recombination}

\author{Sam Toyer\\
The Australian National University\\
{\tt\small u5568237@anu.edu.au}
}

\maketitle
\ifwacvfinal\thispagestyle{empty}\fi

% Random notes about stuff go here.
%
% Figures which I can include:
% - Diagram showing entire pipeline
% - Correct detections
% - Incorrect detections
% - Illustration of recombination candidate set
% - PCK on PIW for elbows/wrists/shoulders, compared with the same papers which
%   Anoop compares with.
%
% Things I can put in the introduction (not all required; just looking through
% other papers to get ideas):
% - What pose estimation is and why it is important.
% - Challenges in pose estimation
% - Explain different classes of approaches to pose estimation (possibly
%   including their disadvantages). Just need to be careful not to tread on the
%   related work section.
% - Abstractly discuss the approach taken in the paper.
% - Summarise experiments performed and the results of those experiments.
%
% Things I can put in the related work section:
% - Talk about existing approaches and how they compare with one another in
%   terms of results and in terms of the computer vision tools used.
% - Explain why my approach is better (technically rather than empirically)
%
% Contents of the intra-frame part:
% - Talk about Yang & Ramanan's articulated model.
% - Discuss the type-based approach to part detection, and its advantages and
%   disadvantages.
% - Mention how the CNN works (esp. the fact that it's a fully convolutional
%   network, and also the way we produce a feature pyramid).
% - Graphical model and inference.
% - Candidate pose generation with NMS at each scale.
% - How do we use distance transforms to make GM inference more efficient? This
%   was something I didn't fully understand, so I might try to write about it in
%   the hope that I come to grok it better.
% - Talk about training.
%
% Contents of the inter-frame part:
% - Just explain how smoothing and recombination works, from optical flow down
%   to producing the final pose.
% - Should also explain whichever extensions I use from Anoop's paper.
% - How do we find our hyperparameters? This is hard, since we don't train
%   hyperparameters; really, we're just fiddling with them until they give us
%   something we want. I could say that I just started with Anoop's parameters,
%   then removed colour tracking and the skin histogram check, which really
%   shows that I have nothing up my sleeves. I could also mention that it's
%   theoretically possible to do a grid search for better parameters (maybe even
%   run such a search and show the results, with the disclaimer that we're
%   training on the test set).
%
% Things to consider putting in the conclusion and future work section (other
% than a summary, obviously):
% - Anoop's suggestion of using a CNN-based regressor to fine-tune joint
%   location estimates. Need to find some prior art on this; did Anoop just make
%   that up when I was talking to him? It seems like a useful insight, so
%   perhaps someone has tried it already.
% - Anoop's other suggestion about using an R-CNN-style approach to CNN forward
%   prop. Not totally convinced about the merits of that (you really only need
%   to identify windows in which there's definitely no human present at all,
%   which isn't what the R-CNN is doing with its semantic segmentation and
%   region joining steps), so I might leave it out.
% - Improve the CNN by using a newer model (e.g. ILSVRC '15 winners) or
%   decreasing the number of output layers from ~10,000 to a few hundred by
%   doing K-means on entire sets of outgoing limbs attached to a single joint.
%   Perhaps there's also a good PCA-based method hiding somewhere in there.
% - Diverse M-best for candidate generation, instead of the crap I'm using at
%   the moment.
% - Using LBP and optical flow to improve pairwise pose estimation.
%
% Remember the Heilmeier Catechism! Jochen said that most of these points should
% be addressed in the discussion:
%
% - What are you trying to do? Articulate your objectives using absolutely no
%   jargon.
% - How is it done today, and what are the limits of current practice?
% - What's new in your approach and why do you think it will be successful?
% - Who cares? If you're successful, what difference will it make?
%
% The Heilmeier Catechism also includes the following questions, although they
% are less relevant for our discussion:
%
% - What are the risks and the payoffs?
% - How much will it cost? How long will it take?
% - What are the midterm and final "exams" to check for success?
%
% The IEEE conference guidelines recommend "Mermin's guide" to typesetting
% mathematics. There are a few interesting rules in it:
% 1) Number everything, whether you intend to refer to it or not. This makes it
%    easier for subsequent readers and people citing your work to find
%    equations, and also makes it easier to describe errata.
% 2) In addition to a number, refer to each equation by some useful name. For
%    example, instead of writing "substituting (7) into (10)", we could write
%    "substituting the temporal cost (7) into the inter-frame cost (10)". This
%    makes it much easier for readers to understand WTF you're talking about.
% 3) End equations with punctuation. Pretend that the equation is prose; if you
%    would normally end that prose with a comma (for example), then you should
%    end the equation with a comma. This rule can be skipped when your equation
%    is embedded in the surrounding text in such a way that, even if it were
%    prose, it would require no trailing punctuation.
% Also, Mermin generally recommends that equations be treated as part of their
% surrounding sentences. If you follow this rule, you won't be including colons
% and the like before equations, nor "introducing" equations ("...as follows:",
% "...the equation:", etc.)

%%%%%%%%% ABSTRACT
\begin{abstract}
    This paper presents a method for estimation of articulated human poses in
    video frames. Our approach is comprised of a Convolutional Neural Network
    (CNN) and tree-structured graphical model for independent generation of
    candidate pose sets in each frame, followed by a recombination step which
    makes use of optical flow and pairwise deformation features to produce a
    single, consistent series of poses across an entire sequence. Evaluation on
    the Poses in the Wild dataset validates the usefulness of inter-frame pose
    recombination over single-frame pose estimation alone, and suggests that our
    method significantly improves upon the state-of-the-art in localising
    difficult body parts like wrists and elbows.
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}

The aim of human pose estimation is to take a sequence of static images or video
frames and output a series of two-dimensional ``skeletons'' representing the
locations of the limbs of any humans in those images. Pose estimation is useful
for higher-level computer vision tasks like identifying clothing
items~\cite{liu2012street,liu2012hi,yamaguchi2012parsing}, recognising
actions~\cite{yao2011does}, or even classifying objects which people interact
with~\cite{delaitre2012scene}.

\section{Related work}

% Papers that I want to cover:
% - Yang & Ramanan
% - Chen & Yuille
% - DeepPose
% - Anoop's paper
% - "Flowing convnets" paper
% - Something on CNN-based part regressors...
% Also some stuff on pictorial structure models (no type information), and any
% more recent stuff that I can find, especially if said stuff is CNN-related.

\section{Single-frame candidate set generation}
\label{sec:intraframe}

Our method for generating a candidate pose set in each frame mostly follows that
of \cyte{chen2014articulated}, although we will describe the relevant parts of
their method in full.

\subsection{Model}
\label{sec:model}

% TODO: Complete figure
\begin{figure}[t]
\begin{center}
\fbox{\rule{0pt}{1.5in} \rule{0.9\linewidth}{0pt}}
\end{center}
\caption{Illustration of the skeleton used to represent candidate poses within
each frame. This figure shows an 18-joint upper-body skeleton, as used for
evaluation in Section~\ref{sec:experiments}}
\label{fig:skeleton}
\end{figure}

Skeletons are represented by a graph consisting of a $\mathcal G = (\mathcal V,
\mathcal E)$ consisting of a set of vertices (also known as ``parts'' or
``joints'') $\mathcal V$ and a set of edges (commonly referred to as ``limbs'')
$\mathcal E \subseteq \mathcal V \times \mathcal V$, as illustrated in
Figure~\ref{fig:skeleton}. A complete pose $\vec{p} = (\vec l, \vec t)$ is
represented by a location $\vec{l_u}$ within an image $\mat I$ for each joint $u
\in \mathcal V$, and discrete ``types'' $t_{uv} \in \{1, \ldots, T_{uv}\}$ and
$t_{vu} \in \{1, \ldots, T_{vu}\}$ for each limb $(u, v) \in \mathcal E$.

Limb types, as popularised by \cyte{yang2011articulated}, are used to express
the orientation and length of limbs. For example, long forearms running from
left to right might be assigned their own discrete type, as might short,
vertical forearms, medium-length diagonal forearms, and so on. During inference,
these types can be introduced as latent variables, in which case they can be
used to encourage the relative positions of body parts to take anatomically
reasonable values through the use of type-dependent limb deformation costs.
Further evidence for the type of a limb can be gleaned by inspecting small
patches of an image around the endpoints of that limb: for instance, an image of
a shoulder might give clues as to the direction in which the attached upper arm
is pointing. Both type-dependent deformation costs and IDPR terms are discussed at
greater length in Section~\ref{sec:img-dep}.

Given a complete pose $(\vec l, \vec t)$ consisting of a vector part locations
$l$ and a vector of limb types $t$, and an image $\mat I$, the full score
$C(\mat l, \mat t \mid \mat I)$ of the pose can be decomposed into a sum of
unary costs and pairwise costs, as in
\begin{equation}
\label{eqn:full-cost}
\begin{split}
C(\vec l, \vec t \mid \mat I)
= w_0 + &\sum_{u \in \mathcal V} \phi_u(\vec l_u \mid \mat I)\\
+ &\sum_{(u, v) \in \mathcal E}
    \psi_{uv}(\vec l_u, \vec l_v, t_{uv}, t_{vu} \mid \mat I),
\end{split}
\end{equation}
where $\phi_u$ and $\psi_{uv}$ are described below, and $w_0$ is a bias term.

\subsection{Image-dependent terms}
\label{sec:img-dep}

The pairwise cost $\phi_{uv}(\vec l_u, \vec l_v, t_{uv}, t_{vu} \mid \mat I)$
can itself be decomposed into the sum of type-dependent deformation costs and
IDPR terms, as expressed by
\begin{equation}
\begin{split}
\psi_{uv}(\vec l_u, \vec l_v, t_{uv}, t_{vu})
= &\vec w_{uvt_{uv}}^T d(\vec l_v - \vec l_u - \vec r_{uv t_{uv}})\\
+ &\vec w_{vut_{vu}}^T d(\vec l_u - \vec l_v - \vec r_{vu t_{vu}})\\
+ &w_{uv} \mathcal I_{uv}(\vec l_u, t_{uv}, \mat I)\\
+ &w_{vu} \mathcal I_{vu}(\vec l_v, t_{vu}, \mat I),
\end{split}
\end{equation}
where $d(\vec v) = \begin{bmatrix}v_x^2 & v_y^2 & v_x & v_y\end{bmatrix}^T$ is a
deformation feature, and $\mathcal I$ represents an IDPR term, explained below.

Given a $K$-joint skeleton, we can define $p(j = u \mid \mat I(\vec l))$ to be
the probability that the joint contained in the image $\mat I(\vec l)$ of $\mat
I$ around $\vec l$ is the joint represented by $u \in \{1, \ldots, K\} \cup
\{0\}$, with the special value of 0 appended to indicate that no joint is
present. If we know that the patch $\mat I(\vec l)$ contains a joint $u$, and
$(u, v) \in \mathcal E$ is a limb, then we can define $p(t_{uv} = t \mid j = u,
\mat I(\vec l))$ to be the probability that the limb between $(u, v)$ has type
$t_{uv} \in \{1, \ldots, T_{uv}\}$. Using this notation, we can define the IDPR
term $\mathcal I$ as
\begin{equation}
\label{eqn:idpr}
\mathcal I_{uv}(\vec l, t, \mat I)
= \log p(t_{uv} = t \mid j = u, \mat I(\vec l)).
\end{equation}

The inclusion of both $\mathcal I_{uv}$ and $\mathcal I_{vu}$ ensures that the
visual cues given by the parts at either end of a limb can be used to infer the
type of that limb.

Similarly, the appearance term $\phi_u$ gives the log likelihood that a small,
fixed-size patch of the image centered at $\vec{l_u}$ contains the joint $u$:

\begin{equation}
\label{eqn:unary}
\phi_u(\vec l \mid \mat I)
= w_u \log p(j = u \mid \mat I(\vec l)).
\end{equation}

\subsection{Computing unaries and IDPR terms}
\label{sec:cnn}

To compute the unaries defined by (\ref{eqn:unary}) and the IDPR terms defined
by (\ref{eqn:idpr}), we train a CNN to output the joint distribution over part
locations and neighbouring limb types for a given image $\mat I$, from which we
may obtain the appearance and IDPR terms by marginalisation. If we let $t_u \in
\prod_{(u, v) \in \mathcal E} \{1, \ldots, T_{uv}\}$ denote a part type---as
opposed to a limb type---which defines a type for all parts neighbouring part
$u$, then we can write this distribution as
\begin{equation}
\label{eqn:cnn-output}
p(j = u, t_u = t \mid \mat I(\vec l))
\end{equation}
for any location $\vec l$ in $\mat I$ and any part $u \in \{1, \ldots, K\}$.

If we discard combinations of $u$ and $t$ for which the joint probability is
always zero, then this becomes a discrete distribution over
\begin{equation}
\sum_{u \in \mathcal V} T^{|\{(u, v) : (u, v) \in \mathcal E\}|} + 1
\end{equation}
values, where $\nb(u)$ denotes the neighbours of part $u$ and we assume that
$T_{uv} = T \in \mathbb N$ for all $(u, v) \in \mathcal E$. The extra value is
for when no part is present ($j = 0$).

Our CNN architecture closely follows that of
AlexNet~\cite{krizhevsky2012imagenet}, and is identical to that of
\cyte{chen2014articulated}. At a high level, the network begins with a
convolution over the input, followed by two convolutional layers coupled with
max pooling and local response normalisation, after which three convolutional
layers are applied before two 4096-neuron fully-connected layers which feed into
the softmax output layer.

To minimise wasted computation, we convert the final fully connected layers of
the network to $1 \times 4096$ convolutions after the network has been trained.
This allows us to evaluate (\ref{eqn:cnn-output}) over a full-resolution image
in a single pass, rather than having to pass appropriately-sized crops of the
image through the network one at a time.

\subsection{Producing the candidate set}

Having evaluated appearance and IDPR terms for all joints and all locations in
the image, we can now produce a set of high-scoring pose candidates for use in
the recombination procedure (Section~\ref{sec:interframe}). Recall from
Section~\ref{sec:model} that poses are modelled as trees rooted at the head. In
this case, the score of any subtree of the full pose tree rooted at part $u$ in
location $\vec l_u$ is:

\begin{equation}\label{eqn:gm-local-score}
\begin{split}
S_u&(\vec l_u, \mat I) =\\
&\sum_{pa(v) = u} \max_{\vec l_v, t_{uv}, t_{vu}} \left[
\psi_{uv}(\vec l_u, \vec l_v, t_{uv}, t_{vu} \mid \mat I)
+ S(\vec l_v, \mat I)
\right]\\
&+ \phi_u(\vec l_u \mid \mat I).
\end{split}
\end{equation}

At the leaves, this formula becomes $S_v(\vec l_v, \mat I) = \phi_v(\vec l_v
\mid \mat I)$, which is trivial to compute for all locations in the image.
Otherwise, given child scores $S_{v_1}(\vec l_{v_1}, \mat I), \ldots,
S_{v_C}(\vec l_{v_C}, \mat I)$ for the children $\{v : \pa(v) = u\}$ of some
non-leaf part $u$, it is possible to evaluate $S_u(\vec l_u, \mat I)$ for all
locations $\vec l_u$ in linear time using a distance
transform~\cite{felzenszwalb2012distance}. If we have $T_{uv} = T$ for each $(u,
v) \in \mathcal E$, then we must also perform maximisation over $T^2$ limb label
combinations at each joint. Since this maximisation must be performed at a total
of $K$ joints, the overall time complexity of calculating the score $S_h(\vec
l_h, \mat I)$ of the root component for all locations $\vec l_h$ is $O(T^2 N
K)$, where $N$ is the number of grid locations.

Given the maximum scores $S_h(\vec l_h, \mat I)$ for a pose rooted at each
possible head location $\vec l_h$, we can produce a set of $M$ pose candidates
by choosing the $M$ highest-scoring head positions and backtracking to find the
remainder of the pose. However, since recombination
benefits from a diverse set of poses, we apply
non-maximum suppression to ensure that our returned candidate pool contains only
poses for which the pairwise intersection-over-union for detected wrists is no
greater than some threshold. This prevents the candidate generation algorithm
from producing a set of poses which differ only by several pixels.

\subsection{Learning}
\label{sec:intra-learning}

Training for the single-frame candidate generation model begins with derivation
of the mean limb deformation $\vec r_{uv t_{uv}}$ for each limb $(u, v) \in
\mathcal E$ and each type $t_{uv} \in \{1, \ldots, T\}$ for that limb, where we
have assumed for simplicity that each limb has the same number of types $T$.
For a limb $(u, v)$, this is achieved by calculating the displacement $\vec l_v
- \vec l_u$ associated with each pose in the training set, then running
$K$-means to find $T$ centroids for the calculated displacements.

Having assigned a type to each limb in the training set, an image crop is made
around each joint and labelled with the joint type and the types of all
neighbouring limbs. This set of examples is augmented rotating all patches
containing a limb through a range of angles, and we also include a set of
patches not containing any people which are labelled with a special negative
label. The produced set of patches and labels is used to find parameters for the
neural network described in Section~\ref{sec:cnn} using a near-identical
training regime to that of \cyte{krizhevsky2012imagenet}.

Finally, we can learn the weights $\{\vec w_{uv t_{uv}}, w_{uv} : (u, v) \in
\mathcal E\} \cup \{w_u : u \in \mathcal V\} \cup \{w_0\}$ of the pose cost
(\ref{eqn:full-cost}). The cost $C(\vec l_n, \vec t_n \mid \mat I_n)$ associated
with the limb locations $\vec l_n$, image $\mat I_n$ and limb types $\vec t_n$
detected from the $n$th training sample can be represented as an inner product
between a weight vector $\vec w$ and a feature vector $\mat \Phi_n$ composed of
all appearance, IDPR and deformation terms, with a constant bias concatenated to
the end. % XXX: When is y_n = 1? When is it -1? WTF?

Thus, we can view this problem as finding the weight vector $\vec w^*$
satisfying the following definition:

\begin{align}
\vec w^* &= \min_{\vec w}\left[ \frac 12 \|\vec w\|^2 + C \sum_n E_n(\vec w)\right]\\
E_n(\vec w) &= \max(0, 1 - y_n \vec w^T \mat \Phi_n)
\end{align}

$\vec w^*$ can be found efficiently using the dual coordinate descent solver of
\cyte{yang2011articulated}.

\section{Pose estimation in videos}
\label{sec:interframe}

% TODO: Complete figure
\begin{figure*}[t]
\begin{center}
\fbox{\rule{0pt}{1in} \rule{0.9\linewidth}{0pt}}
\end{center}
\caption{Diagram showing the recombination process. First, a fixed number of
candidate poses are generated in each frame, sorted descending by score. Next,
non-maximum suppression is performed on wrists and elbows to increase the
diversity of the top $N$ candidates. Finally, the top $N$ candidates in each
frame are fed to the recombination procedure, which greedily recombines limbs in
a top-down fashion whilst trying to minimise (\ref{eqn:recomb-cost}).}
\label{fig:recombination}
\end{figure*}

\subsection{Motivation}

% XXX: This equation has been taken out of the normal page flow so that it
% appears at the bottom of the page where it's referenced. It will almost
% certainly need to be moved before submission. Why LaTeX doesn't have an option
\begin{figure*}[!b]
\centering
\rule{\textwidth}{0.1pt}
\begin{equation}\label{eqn:recomb-cost}
\sum_{t=1}^{F-1} \left[
    C(p_t, I_t)
    + \sum_{u \in \mathcal V}
        \tau_u(\vec l_{u,t}, \vec l_{u,t+1}, \mat I_t, \mat I_{t+1})
    + \sum_{v \in \mathcal V_S} \rho_v(\vec l_{v,t}, \vec l_{v,t}')
\right]
+ C(p_F, I_F) + \sum_{v \in \mathcal V_S} \rho_v(\vec l_{v,F}, \vec l_{v,F}')
\end{equation}
\end{figure*}

We have already seen in Section~\ref{sec:intraframe} how we can use a graphical
model to do pose estimation within a single frame. Pose estimation in videos is
similar to pose estimation in static images, except that we wish to enforce some
sort of temporal consistency between the poses corresponding to successive
frames. The obvious approach to this problem is to apply our existing graphical
model to each frame, but to additionally add some temporal consistency term
$\theta(p_t, p_{t+1}) = \sum_{u \in \mathcal V} \theta_u(l_{u,t}, l_{u,t+1})$, where
$l_{u,t}$ denotes the location of part $u$ at time $t$. If we had $F$ frames in
total, then the full cost would be
\begin{equation}\label{eqn:hypo-interframe}
C(p_F, \mat I_F) + \sum_{t=1}^{F-1}
\left[C(p_t, \mat I_t) + \tau(p_t, p_{t+1})\right].
\end{equation}

(\ref{eqn:hypo-interframe}) corresponds to a complex, highly loopy graph, which
makes it infeasible to find the $p_1, \ldots, p_F$ which minimises
(\ref{eqn:hypo-interframe}) for any nontrivial choice of $\tau$. One way to
reduce this computational burden is to restrict the set of poses which we
consider to some limited set $\mathcal P_t$ in each frame; if we have exactly
$|\mathcal P|$ candidate poses in each frame, then dynamic programming would
allow us to minimise ($\ref{eqn:hypo-interframe}$) in $O(|\mathcal P|^2 F)$
time. $|\mathcal P|^2$ can still be colossal when a large number of poses are
considered in each frame, which restricts the applicability of this technique to
situations in which $|\mathcal P|$ is small.

Unfortunately, whilst a small set of poses might, for each part, contain at
least one pose in which the location of that part is close its best possible
location, it's possible that the set will not contain a single pose in which
\emph{all} parts are close to their best locations.

\subsection{Recombination}

The method in \cyte{cherian2014mixing} avoids the penalty incurred by large
candidate pose sets by taking a small, diverse set of $|\mathcal P|$ candidate
poses and then considering all possible combinations of limbs from each of those
poses. Given $|\mathcal P|$ poses and $K$ limbs, this results in an effective
candidate set of $K^{|\mathcal P|}$ poses in each frame, and ensures that the
best joints in the candidate pose set, rather than just the best poses, are
considered.

Now that we know we can efficiently perform inference on a large effective pose
set, we can introduce temporal smoothing links between each part $u \in \mathcal
V$ in the pose at time $t$ and its counterpart at time $t+1$ using a cost
\begin{equation}\label{eqn:temporal-cost}
\begin{split}
\tau_u&(\vec l_{u,t}, \vec l_{u,t+1}, \mat I_t, \mat I_{t+1})\\
&= \lambda_\tau \|\vec l_{u,t+1}
- \vec l_{u,t}
- f(\vec l_{u,t}, \mat I_t, \mat I_{t+1})\|^2,
\end{split}
\end{equation}
where $f(\vec l_{u,t}, \mat I_t, \mat I_{t+1})$ is the optical flow at location
$\vec l_{u,t}$ between frame $\vec I_t$ and frame $\vec I_{t+1}$.

Additionally, to encourage the connected limbs chosen to make up each frame's
final, recombined pose to be close to one another, we introduce a recombination
cost $\rho_v$ for each pair of limbs $(u, v), (v, w) \in \mathcal E$ (where $u
\neq w$) which share a common part $v$:

\begin{equation}\label{eqn:recomb-strength}
\rho_v(\vec l_v, \vec l_v') = \lambda_\rho \|\vec l_v - \vec l_v'\|^2.
\end{equation}

The complete cost which the recombination process must minimise is therefore
given in (\ref{eqn:recomb-cost}); we have used $\mathcal V_S = \{v : \exists u
\neq w : (u, v) \in \mathcal E \land (v, w) \in \mathcal E\}$ to denote the set
of parts which are shared between two or more limbs, whilst $\vec l_v$ denotes
the location of part $v$ in a recombined pose and $\vec l_v'$ denotes a location
of part a $v$ which was discarded during recombination, but for which the
location of some part $w$ \emph{connected} to $v$ was used.

In order to make the minimisation of the full temporal cost
(\ref{eqn:recomb-cost}) tractable, limbs are recombined starting at the
head---which is typically the easiest part to detect---then moving on to the
neck, the shoulders, and so on until the full pose has been estimated in all
frames.

Specifically, the algorithm starts by choosing a head position $\vec{l_{h,t}}$
at each time $t = 1, \ldots, F$ by choosing a head position from the set
$\mathcal P_t$ in each frame such that the temporal sequence of heads minimises
the following cost, which corresponds to the parts of the full cost
(\ref{eqn:hypo-interframe}) that involve the head or any temporal links between
heads in adjacent frames:

\begin{equation}\label{eqn:head-cost}
\phi_h(\vec l_{h,F}, \mid \mat I_F)
+ \sum_{t=1}^{F-1} \left[
    \phi_h(\vec l_{h,t}, \mid \mat I_t)
    + \tau_h(\vec l_{h,t}, \vec l_{h,t+1})
\right].
\end{equation}

If we have $|\mathcal P|$ candidate poses in each frame, each of which
corresponds to a single head position candidate, then we can use dynamic
programming to perform this minimisation in $O(|\mathcal P|^T F)$ time.

Position sequences for any subsequent part $u$ can be found in much the same
way, except that we must also include pairwise costs from the single-frame cost
$C$, as well as recombination costs relative to the (previously localise) parent
part, yielding a full cost of
\begin{equation}\label{eqn:subsequent-part-cost}
\begin{split}
C_{uv}(l_{u,F}, &\vec l_{v, F}, \vec t \mid \mat I_F)
+ \rho_u(\vec l_{u,F}, \vec l_{u,F}')\\
+ \sum_{t=1}^{F-1} &\bigl[
    C_{uv}(l_{u,t}, \vec l_{v,t}, \vec t \mid \mat I_F)\\
    &\ + \rho_u(\vec l_{u,t}, \vec l_{u,t}') + \tau_u(\vec l_{u,t}, \vec l_{u,t+1})
\bigr],
\end{split}
\end{equation}
where $v = \pa(u)$ is the parent of $u$ and $C_{uv}(\vec l_u, \vec l_v, \vec t
\mid \mat I)$ are the terms of the single-frame cost (\ref{eqn:full-cost}) which
either involve only part $u$ or involve both $u$ and $v$.

As with the head, finding the appropriate sequence of part locations for each
remaining part can be done in $O(|\mathcal P|^2 F)$ time with dynamic
programming, meaning that the total runtime of the minimisation procedure is
$O(K |\mathcal P|^2 F)$ for a $K$-part skeleton.

\subsection{Approximations and heuristics}

In practice, the unary terms in the head sequence cost (\ref{eqn:head-cost}) and
the cost (\ref{eqn:subsequent-part-cost}) for subsequent parts can be
approximated by the full, single-frame inference score $C(\vec l, \vec t \mid
\mat I)$ for the specific candidate pose being considered. This not only makes
implementation easier, but improves performance due to the fact that the
single-frame inference scores are already computed during candidate set
generation.

In addition to the costs listed above, we have used the ``practical extensions''
of \cyte{cherian2014mixing}, which include:

\begin{itemize}
    % Dense temporal regularisation along limbs
    \item Additional temporal regularisation terms in the head cost
    (\ref{eqn:head-cost}) and subsequent part cost
    (\ref{eqn:subsequent-part-cost}) of the form $\tau_{u'}(\vec l_{u',t},
    \vec{l}_{u't+1}, \mat I_t, \mat I_{t+1})$, where $\vec l_{u'}$ is is a
    position at a fixed distance along some specific limb. These additional
    regularisation terms could be introduced to penalise the motion of the
    midpoint of the forearm, for instance, without having to introduce an
    entirely new joint at the midpoint of the forearm.

    % Enriched wrists
    \item A term $\lambda_f F_t^{-1} \|f(\vec l_{w,t}, \mat I_t, \mat
    I_{t+1})\|$ for each wrist $w$, where $F_t$ is the maximum magnitude of the
    optical flow between frames $\mat I_t$ and $\mat I_{t+1}$. This encourages
    wrists to occupy regions of high motion.

    To combine this extension with the previous one, we can replace the
    magnitude of the flow at a wrist with sum of the magnitudes of the flow at
    that wrist and at all intermediate temporal regularisation points between
    the wrist and its parent joint.

    % Limiting motion
    \item A regularisation term of the form $\lambda_m \|\vec l_{u,t} -
    \vec{l}_{u,t+1}\|^2$ which can be used to reduce the impact of erroneously
    large flow vectors.
\end{itemize}

Note that $\lambda_\tau$, $\lambda_\rho$, $\lambda_f$ and $\lambda_m$ are all
challenging to learn in a structured way. We have opted to tune the
recombination parameters in \cyte{cherian2014mixing} by hand, although in
principle it should also be possible to obtain reasonable results with grid
search.

\section{Experiments}
\label{sec:experiments}

% TODO: Complete figure
\begin{figure*}[t]
\begin{center}
\fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
\end{center}
\caption{Comparison of accurate and inaccurate pose estimates. The top row shows
several consecutive frames of a challenging sequence on which the model performs
well. The top row shows a number of individual frames in which the model
performed poorly.}
\label{fig:qualitative}
\end{figure*}

Code for these experiments is available
online.\footnote{\url{https://github.com/qxcv/comp2560}}

\subsection{Multi-scale detection}

Since training (Section~\ref{sec:intra-learning}) is performed on images which
have an (approximately) equal scale, we have found it advantageous to perform
candidate set generation at several scales. We perform multi-scale detection on
each test frame by first passing the test frame through our CNN at a series of
fixed scales to produce a feature pyramid of probabilities for part presence and
limb types. Graphical model inference is then performed independently at each
scale, and the set of % TODO

Note that, since we have only evaluated our full pipeline on the approximately
uniformly-scaled poses in Poses in the Wild, we have not adjusted the
recombination stage to account for multiple scales.

\subsection{Discussion}

\subsection{Future work}

\section{Conclusion}

\ifwacvfinal{%
\noindent\textbf{Acknowledgements}\quad
We would like to thank the authors of \cyte{chen2014articulated} and
\cyte{cherian2014mixing} for making their code publicly available.
}\fi

{\small
\bibliographystyle{latex-kit/ieee}
\bibliography{citations}
}
\end{document}
