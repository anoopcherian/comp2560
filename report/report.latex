\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{latex-kit/wacv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{cite}
% This makes floats actually float in the right god-damn place
\usepackage{dblfloatfix}
\usepackage{microtype}
\usepackage{siunitx}
\usepackage{pgf}
\usepackage[percent]{overpic}
\usepackage{color}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

% IMPORTANT! Omitting \wacvfinalcopy will leave rulers on the side, include a
% paper ID, disable acknowledgements, hide the authors list, etc. (as
% appropriate for reviewing)
{\wacvfinalcopy}

\def\wacvPaperID{0000} % Fake paper ID for the quasi-conference
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ifwacvfinal\pagestyle{empty}\fi
\setcounter{page}{1}

% Macros which I find useful:
\renewcommand{\vec}{\mathbf}
\newcommand{\mat}{\mathbf}
\DeclareMathOperator{\nb}{nb}
\DeclareMathOperator{\pa}{pa}
\DeclareMathOperator{\ch}{ch}
% This is just to stop Syntastic from complaining. Grumble grumble
% fixing your tools...
\newcommand{\cyte}[1]{\cite{#1}}

\begin{document}

\title{Articulated Pose Estimation in Videos with Convolutional Neural Networks
and Pose Recombination}

\author{Sam Toyer\\
The Australian National University\\
{\tt\small u5568237@anu.edu.au}}

\maketitle
\ifwacvfinal\thispagestyle{empty}\fi

% Random notes about stuff go here.
%
% Remember the Heilmeier Catechism! Jochen said that most of its points should
% be addressed in the discussion. I'll keep track of what I have addressed:
%
% - What are you trying to do? Articulate your objectives using absolutely no
%   jargon.
%   --> Did this in the intro! I have a pretty diagram of our pose detector and
%       everything.
% - How is it done today, and what are the limits of current practice?
%   --> Sort-of did this in related work. Not sure that fully qualifies, though,
%       because I don't talk about what specific limitations which my algorithm
%       addresses.
%   --> Actually, looking at the introduction and related work sections, I
%       really do address this. I just feel uneasy about not talking about the
%       Big Problems in pose estimation, since I feel as though "limitations of
%       current practice" should refer to the limitations of the field *as a
%       whole*, not the limitations of specific algorithms. My algorithm doesn't
%       really address any of open HPE problems, or improve on the state of the
%       art (which is an implicit limitation of the field as a whole), so I'm
%       not sure what to write there.
% - What's new in your approach and why do you think it will be successful?
%   --> Need to do this. Could really merge it with the previous part. Heck, I
%       could even put it in the abstract.
%   ---> Yeah, I sort-of do this already. Perhaps saying "we extend <blah> with
%        <blah>, which to the best of our knowledge is novel" might help.
% - Who cares? If you're successful, what difference will it make?
%   --> Kind-of did this in the intro with the "what are you trying to do" bit.
%
% The Heilmeier Catechism also includes the following questions, although they
% are less relevant for our discussion:
%
% - What are the risks and the payoffs?
% - How much will it cost? How long will it take?
% - What are the midterm and final "exams" to check for success?
%
% Random note on first-person pronouns: in COMP2550, I used "royal we"
% (referring to yourself as "we"), and it worked okay, so I think I'll do that
% again for this paper. I'm unnaturally attached to scientific "we", so I don't
% want to throw it away :-)

\begin{abstract}
    This paper presents a method for 2D human pose estimation across a sequence
    of video frames. Our approach is comprised of a Convolutional Neural Network
    (CNN) and graphical model for independent generation of pose candidate sets
    in each frame, followed by a recombination step which makes use of optical
    flow and limb deformation costs to produce a single, consistent sequence of
    poses. Evaluation on the Poses in the Wild data set validates the usefulness
    of inter-frame pose recombination over single-frame pose estimation alone,
    and shows that our method significantly improves upon past work in
    localisation of wrists and elbows.
\end{abstract}

\section{Introduction}
\label{sec:introduction}

The aim of human pose estimation is to take a sequence of static images or video
frames and output a series of two-dimensional ``skeletons'' representing the
locations of the limbs of any humans in those images, as depicted in
Figure~\ref{fig:skeleton}. Skeletons produced through pose estimation are useful
for higher-level computer vision tasks like identifying clothing
items~\cite{liu2012street,liu2012hi,yamaguchi2012parsing}, recognising
actions~\cite{yao2011does}, and even classifying inanimate objects through a
person's interactions with them~\cite{delaitre2012scene}.

Pose estimation is complicated by the wide range of anatomically possible human
poses, the varied appearance of human bodies and clothing, occlusion of
joints by objects in the scene or by other limbs, background clutter which
resembles joints, and so on. Taking advantage of the motion information
present in video sequences introduces another layer of complexity to this
already challenging problem.

We deal with this complexity by dividing our video pose estimation pipeline into
two phases. Initially, a pose estimation procedure is applied independently to
each frame in the video sequence. This procedure implicitly scores all possible
poses in each video frame and returns a selection of the highest-scoring poses,
which is pruned through non-maximum suppression to ensure that the returned set
contains a diverse selection of poses. After a candidate pose set has been
generated for each frame, the joints associated with each pose candidate in each
frame are recombined to reduce each frame's candidate set down to a single
high-scoring pose.

\begin{figure}[t]
\begin{center}
\begin{overpic}[width=0.33\linewidth]{good-shots/frame-1.png}
    \put (90, 80) {1}
\end{overpic}%
\begin{overpic}[width=0.33\linewidth]{good-shots/frame-2.png}
    \put (90, 80) {2}
\end{overpic}%
\begin{overpic}[width=0.33\linewidth]{good-shots/frame-3.png}
    \put (90, 80) {3}
\end{overpic}
% \begin{overpic}[width=0.24\linewidth]{good-shots/frame-4.png}
%     \put (90, 80) {4}
% \end{overpic}
\end{center}
\caption{Excerpt from a video sequence annotated with the detections produced by
our algorithm. Only shoulders, wrists and elbows are shown in this example.}
\label{fig:skeleton}
\end{figure}

Our candidate set generation procedure, described in
Section~\ref{sec:intraframe}, follows the approach of Chen and
Yuille~\cite{chen2014articulated}, which combines features from a deep CNN with
a graphical model to estimate poses in a static image. The CNN yields features
which are much more informative than the Histogram-of-Gradients (HoG) features
used in previous
work~\cite{yang2011articulated,cherian2014mixing,ferrari2008progressive,sapp2013modec},
whilst the graphical model encourages joints to take on
anatomically reasonable relative positions.

As explained in Section~\ref{sec:interframe}, incorporating temporal edges into
graphical model-based pose estimation methods can result in intractable
inference problems because of the loopiness of the resulting graph.
Independently generating a small set of candidate poses in each frame and using
temporal edges to select a candidate from each frame partially alleviates this
problem by reducing the number of poses which must be considered. The
recombination method which we employ, as described by Cherian
\etal~\cite{cherian2014mixing}, improves efficiency even further by allowing
joints to be mixed-and-matched from different poses in each candidate set in a
top-down fashion, thereby increasing the effective size of the candidate sets
under consideration whilst maintaining low algorithmic complexity.

\section{Related work}

Early attempts at human pose estimation often made use of pictorial structure
models~\cite{fischler1973representation,felzenszwalb2005pictorial}, which
exploit the fact that poses can be decomposed into joints which are likely to
stay within a certain range of distances of one another. Yang and
Ramanan~\cite{yang2011articulated} extended this approach by introducing the
notion of part types to capture the different appearances which a part can have
depending on whether it's open or closed (in the case of hands), whether it's
extended or folded (arms), how much it is rotated, and so on. By treating the
types of each part as latent variables in their graphical model, and augmenting
their deformation terms to take into account the joints on either end of a limb,
Yang and Ramanan were able to obtain a significant increase in accuracy over
traditional pictorial structure models. In this paper, we use a similar approach
for generating pose candidate sets within frames.

Another common approach is to regress the $(x, y)$ coordinates of joints directly
from an image~\cite{toshev2014deeppose,sun2012conditional}, although this can
lead to implausible poses being generated unless some constraints on the
relative positions of limbs or on the overall pose are introduced. Use of
graphical models can increase accuracy by eliminating pose candidates in which
predicted joint positions are individually likely, but collectively implausible
due to the anatomical constraints imposed by human limbs.

More recently, deep neural networks have been successfully applied to pose
estimation in a variety of ways. One common approach is to attempt to regress
joint coordinates directly; Toshev and Szegedy~\cite{toshev2014deeppose} do this
with a cascade of regressors, where the first regressor outputs an approximate
location for each joint, and subsequent regressors are used to refine those
approximations. Alternative
approaches~\cite{jain2013learning,jain2014modeep,pfister2015flowing} instead map
images to heatmaps for each joint, which has the advantage of being a less
nonlinear mapping than that from images to joint coordinates. The heatmap
approach also makes it straightforward to incorporate graphical models later in
the pose estimation pipeline, since the heatmap values at different locations
can be used as potentials for graphical model
inference~\cite{chen2014articulated,tompson2014joint}, as explained further in
Section~\ref{sec:img-dep}.

The motion information available in videos has previously been exploited by
tracking detected poses throughout a video
sequence~\cite{ji20023d,andriluka2010monocular} or by extending graphical models
for single-frame prediction with temporal links between joints in different
frames~\cite{ferrari2008progressive,cherian2014mixing,sigal2004tracking}. Motion
information has also been incorporated into CNN-based detectors by introducing
motion features like optical flow at the input layers, which can sometimes
increase accuracy over single-frame pose estimation~\cite{jain2014modeep}. As
alluded to in Section~\ref{sec:introduction}, incorporating temporal links into
graphical models tends to result in intractable inference problems, and so past
approaches in this area have generally used approximate methods for inference.
We avoid this problem by generating pose candidate sets independently and taking
the recombination approach described in
Section~\ref{sec:introduction}~\cite{cherian2014mixing}.

\section{Single-frame candidate set generation}
\label{sec:intraframe}

\subsection{Model}
\label{sec:model}

Our method for generating pose candidate sets largely follows that of
\cyte{chen2014articulated}, although we will use this section to describe the
relevant parts of their method in full.

Pose skeletons are represented by a graph $\mathcal G = (\mathcal V, \mathcal
E)$ consisting of a set of vertices (or ``joints'') $\mathcal V$ and a set of
edges (or ``limbs'') $\mathcal E
\subseteq \mathcal V \times \mathcal V$.\footnotemark A complete pose $\vec{p} =
(\vec l, \vec t)$ is represented by a location $\vec{l_u}$ within an image $\mat
I$ for each joint $u \in \mathcal V$, and discrete ``types'' $t_{uv} \in \{1,
\ldots, T_{uv}\}$ and $t_{vu} \in \{1, \ldots, T_{vu}\}$ for each limb $(u, v)
\in \mathcal E$.

\footnotetext{Not all vertices in the pose graph correspond to physical joints,
and not all edges correspond to physical limbs, but we will stick with this
anatomically incorrect nomenclature for the sake of consistency.}

Limb types are used to express the orientation and length of limbs. For example,
long forearms running from left to right might be assigned their own discrete
type, as might short, vertical forearms, medium-length diagonal forearms, and so
on. During inference, these types are introduced as latent variables, and
type-dependent limb deformation costs are added to encourage joints to take
anatomically reasonable relative positions. Further evidence for the type of a
limb can be gleaned by inspecting small patches of an image around the endpoints
of that limb. For instance, an image of a shoulder might give clues as to the
direction in which the attached upper arm is pointing; this notion is the
motivating force behind Image-Dependent Pairwise Relations (IDPRs), which are
discussed at greater length in Section~\ref{sec:img-dep}.

Given an image $\mat I$ and a complete pose $(\vec l, \vec t)$ consisting of a
set of part locations $\vec l$ and a set of limb types $\vec t$, the full score
$C(\mat l, \mat t \mid \mat I)$ of the pose can be decomposed into a sum of
unary costs and pairwise costs, written as
\begin{equation}
\label{eqn:full-cost}
\begin{split}
C(\vec l, \vec t \mid \mat I)
= w_0 + &\sum_{u \in \mathcal V} \phi_u(\vec l_u \mid \mat I)\\
+ &\sum_{(u, v) \in \mathcal E}
    \psi_{uv}(\vec l_u, \vec l_v, t_{uv}, t_{vu} \mid \mat I),
\end{split}
\end{equation}
where $\phi_u$ and $\psi_{uv}$ are described in the next section, and $w_0$ is a
bias term.

\subsection{Image-dependent terms}
\label{sec:img-dep}

The pairwise cost $\psi_{uv}(\vec l_u, \vec l_v, t_{uv}, t_{vu} \mid \mat I)$
can be decomposed into the sum of type-dependent deformation costs and IDPR
terms, as expressed by
\begin{equation}
\begin{split}
\psi_{uv}(\vec l_u, \vec l_v, t_{uv}, t_{vu})
= &\vec w_{uvt_{uv}}^T d(\vec l_v - \vec l_u - \vec r_{uv t_{uv}})\\
+ &\vec w_{vut_{vu}}^T d(\vec l_u - \vec l_v - \vec r_{vu t_{vu}})\\
+ &w_{uv} \mathcal I_{uv}(\vec l_u, t_{uv}, \mat I)\\
+ &w_{vu} \mathcal I_{vu}(\vec l_v, t_{vu}, \mat I),
\end{split}
\end{equation}
where $d(\vec v) = \begin{bmatrix}v_x^2 & v_y^2 & v_x & v_y\end{bmatrix}^T$ is a
deformation feature, and $\mathcal I$ represents an IDPR term, explained below.

Given a $K$-joint skeleton, we can define $p(j = u \mid \mat I(\vec l))$ to be
the probability that the joint contained in the patch $\mat I(\vec l)$ of $\mat
I$ around $\vec l$ is the joint represented by $u \in \{1, \ldots, K\} \cup
\{0\} = \mathcal V \cup \{0\}$, with the special value of $u = 0$ indicating
that no joint is present. If we know that the patch $\mat I(\vec l)$ contains a
joint $u$, and $(u, v) \in \mathcal E$ is a limb, then we can define $p(t_{uv} =
t \mid j = u, \mat I(\vec l))$ to be the probability that the limb between $(u,
v)$ has type $t_{uv} \in \{1, \ldots, T_{uv}\}$. Using this notation, we can
define the IDPR term $\mathcal I$ as
\begin{equation}
\label{eqn:idpr}
\mathcal I_{uv}(\vec l_u, t, \mat I)
= \log p(t_{uv} = t \mid j = u, \mat I(\vec l_u)).
\end{equation}

The inclusion of both $\mathcal I_{uv}$ and $\mathcal I_{vu}$ ensures that the
visual cues given by the joints at either end of a limb can be used to infer the
type of that limb.

The appearance term $\phi_u$ is defined similarly, and gives the log probability
that a small, fixed-size patch of the image centered at $\vec{l}_u$ contains the
joint $u$:
\begin{equation}
\label{eqn:unary}
\phi_u(\vec l_u \mid \mat I)
= w_u \log p(j = u \mid \mat I(\vec l_u)).
\end{equation}

\subsection{Computing unaries and IDPR terms}
\label{sec:cnn}

To compute the unaries defined by (\ref{eqn:unary}) and the IDPR terms defined
by (\ref{eqn:idpr}), we train a CNN to output the joint distribution over part
locations and neighbouring limb types for each patch of a given image $\mat I$,
from which we may obtain the appearance and IDPR terms by marginalisation. If we
let $t_u \in \prod_{(u, v) \in \mathcal E} \{1, \ldots, T_{uv}\}$ denote a
combination of types for all limbs adjacent to a part $u$, then we can write
this distribution as
\begin{equation}
\label{eqn:cnn-output}
p(j = u, t_u = t \mid \mat I(\vec l))
\end{equation}
for any location $\vec l$ in $\mat I$ and any part $u \in \{1, \ldots, K\}$.

If we disregard combinations of $u$ and $t$ for which the joint probability is
always zero, then this becomes a discrete distribution over
\begin{equation}
\sum_{u \in \mathcal V} T^{|\{(u, v) : (u, v) \in \mathcal E\}|} + 1
\end{equation}
values, where we assume that $T_{uv} = T \in \mathbb N$ for all $(u, v) \in
\mathcal E$.

Our CNN architecture closely follows that of
AlexNet~\cite{krizhevsky2012imagenet}, and is identical to that of
\cyte{chen2014articulated}. To minimise wasted computation, we convert the final
fully connected layers of the network to $1 \times 4096$ convolutions after the
network has been trained. This allows us to evaluate (\ref{eqn:cnn-output}) over
a set of uniformly spaced patches of a full-resolution image in a single pass,
rather than having to pass patches through the network one at a
time~\cite{sermanet2013overfeat}.

\subsection{Producing the candidate set}

Having evaluated appearance and IDPR terms for all joints and all locations in
the image, we can now produce a set of high-scoring pose candidates for use in
the recombination procedure (Section~\ref{sec:interframe}). Recall from
Section~\ref{sec:model} that poses are modelled as trees rooted at the head. In
this case, the score of any subtree of the full pose tree rooted at part $u$ in
location $\vec l_u$ is
\begin{equation}\label{eqn:gm-local-score}
\begin{split}
S_u&(\vec l_u, \mat I) =\\
&\sum_{pa(v) = u} \max_{\vec l_v, t_{uv}, t_{vu}} \left[
\psi_{uv}(\vec l_u, \vec l_v, t_{uv}, t_{vu} \mid \mat I)
+ S(\vec l_v, \mat I)
\right]\\
&+ \phi_u(\vec l_u \mid \mat I),
\end{split}
\end{equation}
where $\pa(v) = u$ iff $u$ is the parent of $v$ in the full pose tree.

At the leaves, this formula becomes $S_v(\vec l_v, \mat I) = \phi_v(\vec l_v
\mid \mat I)$, which is trivial to compute for all locations in the image.
Otherwise, given child scores $S_{v_1}(\vec l_{v_1}, \mat I), \ldots,
S_{v_C}(\vec l_{v_C}, \mat I)$ for the children $\{v : \pa(v) = u\}$ of some
non-leaf part $u$, it is possible to evaluate $S_u(\vec l_u, \mat I)$ for all
locations $\vec l_u$ in linear time using distance
transforms~\cite{felzenszwalb2012distance}. If we have $T_{uv} = T$ for each
$(u, v) \in \mathcal E$, then we must also perform maximisation over $T^2$ limb
label combinations at each joint. Since this maximisation must be performed at a
total of $K$ joints, the overall time complexity of calculating the score
$S_h(\vec{l}_h, \mat I)$ of the root component for all locations $\vec l_h$ is
$O(T^2 N K)$, where $N$ is the number of grid locations.

Given the maximum scores $S_h(\vec l_h, \mat I)$ for a pose rooted at each
possible head location $\vec l_h$, we can produce a set of $M$ pose candidates
by choosing the $M$ highest-scoring head positions and backtracking to find the
remainder of the pose. However, since recombination
benefits from a diverse set of poses, we apply
non-maximum suppression to ensure that our returned candidate pool contains only
poses for which the pairwise intersection-over-union for detected wrists is no
greater than some threshold.

\subsection{Learning}
\label{sec:intra-learning}

Training for the single-frame candidate generation model begins with derivation
of the mean limb displacement $\vec r_{uv t_{uv}}$ for each limb $(u, v) \in
\mathcal E$ and each type $t_{uv} \in \{1, \ldots, T\}$ for that limb, where we
have assumed for simplicity that each limb has the same number of types $T$. For
a limb $(u, v)$, this is achieved by calculating the displacement $\vec l_v -
\vec l_u$ associated with each pose in the training set, then running $K$-means
to find $T$ centroids for the calculated displacements.

Having assigned a type to each limb in the training set, an image crop is made
around each joint and labelled with the joint type and the types of all
neighbouring limbs. We also include a set of patches not containing any people,
which are labelled with a special negative label. The produced set of patches
and labels is used to find parameters for the neural network described in
Section~\ref{sec:cnn} using stochastic gradient descent.

Finally, we can learn the bias $w_0$ and the weight sets $\{\vec w_{uv
t_{uv}}\}$, $\{w_{uv}\}$, and $\{w_u\}$ for the pose cost (\ref{eqn:full-cost}).
The cost $C(\vec l_n, \vec t_n \mid \mat I_n)$ associated with the limb
locations $\vec l_n$, image $\mat I_n$ and limb types $\vec t_n$ detected from
the $n$th training sample can be represented as an inner product between a
weight vector $\vec w$ and a feature vector $\mat \Phi_n$ composed of all
appearance, IDPR and deformation terms, with a constant bias concatenated to the
end. We label the $n$th training sample with $y_n = 1$ if the training sample
corresponds to a labelled pose with correctly detected type labels, and $y_n =
-1$ if the training sample corresponds to a negative image with no pose or if
the type labels are not correctly inferred. We can now cast learning of the
graphical model weights as an optimisation problem in which we must find the
weight vector $\vec w^*$ satisfying
\begin{align}
\vec w^* &= \min_{\vec w}\left[ \frac 12 \|\vec w\|^2 + C \sum_n E_n(\vec w)\right],\\
E_n(\vec w) &= \max(0, 1 - y_n \vec w^T \mat \Phi_n).
\end{align}

We calculate $\vec w^*$ efficiently using the dual coordinate descent approach
described in \cyte{yang2011articulated}.

\section{Pose estimation in videos}
\label{sec:interframe}

\subsection{Motivation}

We have already seen in Section~\ref{sec:intraframe} how we can use a graphical
model to do pose estimation within a single frame. Pose estimation in videos is
similar to pose estimation in static images, except that we wish to enforce some
sort of temporal consistency between poses. The obvious approach to this problem
is to apply our existing graphical model to each frame, but to also add some
temporal consistency term $\tau(p_t, p_{t+1}) = \sum_{u \in \mathcal V}
\tau_u(\vec l_{u,t}, \vec l_{u,t+1})$, where $\vec l_{u,t}$ denotes the location
of part $u$ at time $t$. If we had $F$ frames in total, then the full cost would
be
\begin{equation}\label{eqn:hypo-interframe}
C(p_F, \mat I_F) + \sum_{t=1}^{F-1}
\left[C(p_t, \mat I_t) + \tau(p_t, p_{t+1})\right].
\end{equation}

(\ref{eqn:hypo-interframe}) corresponds to a complex, highly loopy graph, which
makes it infeasible to find the $p_1, \ldots, p_F$ which minimises
(\ref{eqn:hypo-interframe}) for any nontrivial choice of $\tau$. One way to
reduce this computational burden is to restrict the set of poses which we
consider to some limited set $\mathcal P_t$ in each frame; if we have exactly
$|\mathcal P|$ candidate poses in each frame, then dynamic programming would
allow us to minimise ($\ref{eqn:hypo-interframe}$) in $O(|\mathcal P|^2 F)$
time. $|\mathcal P|^2$ can still be colossal when a large number of poses are
considered in each frame, which restricts the applicability of this technique to
situations in which $|\mathcal P|$ is small.

\subsection{Recombination}

The method in \cyte{cherian2014mixing} avoids the penalty incurred by large pose
candidate sets by taking a small, diverse set of $|\mathcal P|$ candidate poses
and then considering all possible combinations of joints from each of those
poses. Given $|\mathcal P|$ poses and $K$ joints, this results in an effective
candidate set of ${|\mathcal P|}^K$ poses in each frame, and ensures that the
best joints in the pose candidate set, rather than just the best candidates, are
considered.

Now that we know we can efficiently perform inference on a large effective pose
set, we can introduce temporal smoothing links between each part $u \in \mathcal
V$ in the pose at time $t$ and its counterpart at time $t+1$ using a cost
\begin{equation}\label{eqn:temporal-cost}
\begin{split}
\tau_u&(\vec l_{u,t}, \vec l_{u,t+1}, \mat I_t, \mat I_{t+1})\\
&= \lambda_\tau \|\vec l_{u,t+1}
- \vec l_{u,t}
- f(\vec l_{u,t}, \mat I_t, \mat I_{t+1})\|^2,
\end{split}
\end{equation}
where $f(\vec l_{u,t}, \mat I_t, \mat I_{t+1})$ is the optical flow at location
$\vec l_{u,t}$ between frame $\vec I_t$ and frame $\vec I_{t+1}$.

Additionally, to encourage the connected limbs chosen to make up each frame's
final, recombined pose to be close to one another, we introduce a recombination
cost $\rho_v$ for each pair of limbs $(u, v), (v, w) \in \mathcal E$ (where $u
\neq w$) which share a common part $v$:

\begin{equation}\label{eqn:recomb-strength}
\rho_v(\vec l_v, \vec l_v') = \lambda_\rho \|\vec l_v - \vec l_v'\|^2.
\end{equation}

The complete cost which the recombination process must minimise is therefore
given in (\ref{eqn:recomb-cost}); we have used $\mathcal V_S = \{v : \exists u
\neq w : (u, v) \in \mathcal E \land (v, w) \in \mathcal E\}$ to denote the set
of joints which are shared between two or more limbs, whilst $\vec l_v$ denotes
the location of part $v$ in a recombined pose and $\vec l_v'$ denotes a location
of part a $v$ which was discarded during recombination, but for which the
location of some part $w$ \emph{connected} to $v$ was used.

\begin{equation}\label{eqn:recomb-cost}
\begin{split}
C(p_F, \mat I_F) &+ \sum_{v \in \mathcal V_S} \rho_v(\vec l_{v,F}, \vec
l_{v,F}')\\
+ \sum_{t=1}^{F-1} \biggl[
    &C(p_t, \mat I_t)
    + \sum_{v \in \mathcal V_S} \rho_v(\vec l_{v,t}, \vec l_{v,t}')\\
    &+ \sum_{u \in \mathcal V}
        \tau_u(\vec l_{u,t}, \vec l_{u,t+1}, \mat I_t, \mat I_{t+1})
\biggr]
\end{split}
\end{equation}

In order to make the minimisation of the full temporal cost
(\ref{eqn:recomb-cost}) tractable, limbs are recombined starting at the
head---which is typically the easiest joint to detect---then moving on to the
neck, the shoulders, and so on until the full pose has been estimated in all
frames.

Specifically, the algorithm starts by choosing a head position $\vec l_{h,t}$ at
each time $t = 1, \ldots, F$ by choosing a head position from the set $\mathcal
P_t$ in each frame such that the chosen sequence of heads minimises the
following cost, which corresponds to the parts of the full cost
(\ref{eqn:hypo-interframe}) that involve the head or any temporal links between
heads in adjacent frames:
\begin{equation}\label{eqn:head-cost}
\phi_h(\vec l_{h,F}, \mid \mat I_F)
+ \sum_{t=1}^{F-1} \left[
    \phi_h(\vec l_{h,t}, \mid \mat I_t)
    + \tau_h(\vec l_{h,t}, \vec l_{h,t+1})
\right].
\end{equation}

If we have $|\mathcal P|$ candidate poses in each frame, each of which
corresponds to a single head position candidate, then we can use dynamic
programming to perform this minimisation in $O(|\mathcal P|^2 F)$ time.

Position sequences for any subsequent joint $u$ can be found in much the same
way, except that we must also include pairwise costs from the single-frame cost
$C$, as well as recombination costs relative to the (previously localised) parent
joint, yielding a full cost of
\begin{equation}\label{eqn:subsequent-part-cost}
\begin{split}
C_{uv}(l_{u,F}, &\vec l_{v, F}, \vec t \mid \mat I_F)
+ \rho_u(\vec l_{u,F}, \vec l_{u,F}')\\
+ \sum_{t=1}^{F-1} &\bigl[
    C_{uv}(l_{u,t}, \vec l_{v,t}, \vec t \mid \mat I_F)\\
    &\ + \rho_u(\vec l_{u,t}, \vec l_{u,t}') + \tau_u(\vec l_{u,t}, \vec l_{u,t+1})
\bigr],
\end{split}
\end{equation}
where $v = \pa(u)$ is the parent of $u$ and $C_{uv}(\vec l_u, \vec l_v, \vec t
\mid \mat I)$ are the terms of the single-frame cost (\ref{eqn:full-cost}) which
either involve only joint $u$ or involve both $u$ and $v$.

As with the head, finding the appropriate sequence of joint locations for each
remaining joint can be done in $O(|\mathcal P|^2 F)$ time with dynamic
programming, meaning that the total runtime of the minimisation procedure is
$O(K |\mathcal P|^2 F)$ for a $K$-joint skeleton.

\subsection{Approximations and heuristics}

In practice, the unary terms in the head sequence cost (\ref{eqn:head-cost}) and
the cost (\ref{eqn:subsequent-part-cost}) for subsequent joints can be
approximated by the full, single-frame inference score $C(\vec l, \vec t \mid
\mat I)$ for the specific candidate pose being considered. This not only makes
implementation easier, but improves performance due to the fact that the
single-frame inference scores are already computed during candidate set
generation.

In addition to the costs listed above, we have used the ``practical extensions''
of \cyte{cherian2014mixing}, which include additional keypoints along limbs to
constrain motion further, an additional term for wrists which encourages them to
occupy regions of high motion, and a regularisation term which acts to constrain
the absolute difference between joint positions across frames.

% NOTE: This has been taken out of the normal page flow so that it appears in the
% right place. This should be corrected before submission.
\begin{figure*}[ht!]
\begin{center}
\begin{tabular}{@{}c@{}c@{}c@{}c@{}c@{}c@{}}
\includegraphics[height=0.14\linewidth]{bad-shots/seq-50-frame-26.png}\,&
\includegraphics[height=0.14\linewidth]{bad-shots/seq-70-frame-30.png}\,&
\includegraphics[height=0.14\linewidth]{bad-shots/seq-62-frame-2.png}\,&
\includegraphics[height=0.14\linewidth]{bad-shots/seq-63-frame-9.png}\,&
\includegraphics[height=0.14\linewidth]{bad-shots/seq-39-frame-14.png}\,&
\includegraphics[height=0.14\linewidth]{bad-shots/seq-73-frame-30.png}\\
(a) & (b) & (c) & (d) & (e) & (f)
\end{tabular}
\end{center}
\caption{Common types of errors encountered during testing on the Poses in the
Wild data set.}
\label{fig:qualitative}
\end{figure*}

\begin{figure*}[ht!]
\begin{center}
\input{plots/accuracy.pgf}
\end{center}
\caption{PCK over the entire Poses in the Wild data set for different joints.
A joint is considered to be correctly detected in a single frame if the distance
between its predicted position and its true position is less than the pixel
distance threshold indicated on the $x$ axis of the graph. Note that the curve
for the default configuration in the shoulder plot is hidden by that of the
static-only configuration.}
\label{fig:quantitative}
\end{figure*}

\section{Experiments}
\label{sec:experiments}

\subsection{Setup}

To evaluate the performance of our model, we tested it on the Poses in the
Wild~\cite{cherian2014mixing} data set, which consists of a series of 16--30
frame sequences extracted from movies. Many sequences in this data set include a
large degree of camera motion, rapidly moving subjects, cluttered backgrounds or
occlusion of joints.

The single-frame pose estimation model was trained on the Frames Labelled in
Cinema (FLIC) data set~\cite{sapp2013modec}, with negatives drawn from the INRIA
person data set.{\footnotemark} The data set was augmented by rotating images
through a \SI{70}{\degree} range in \SI{5}{\degree} increments, as done in
\cyte{chen2014articulated}.

\footnotetext{\url{http://pascal.inrialpes.fr/data/human/}}

For recombination, we chose a set of hand-tuned parameters which differed for
each pair of joints, depending on the extent of motion of the joints. The precise
set of parameters are available online, along with the rest of the code for
these experiments.\footnote{\url{https://github.com/qxcv/comp2560}} At test
time, 100 candidate poses are used in each frame and NMS is performed at a
threshold of 95\% of the intersection-over-union on each wrist.

We have found it advantageous to perform candidate set generation independently
at several scales. The highest-scoring poses over all scales were then passed to
the recombination stage to produce a final pose sequence.

\subsection{Results}

The mean Percentage Correct Keypoints (PCK) for our algorithm, averaged over all
sequences in Poses in the Wild, is given in Figure~\ref{fig:qualitative}. For
comparison, we have included the results of our algorithm in its previously
described evaluation configuration (``Default config.'') and the results of our
algorithm when only one candidate pose is generated for each frame
(``Static-only''). For comparison, we have also included the results of Cherian
\etal~\cite{cherian2014mixing} on the same sequence.
Timings for the different stages of the pose estimation pipeline during testing
on a single sequence of Poses in the Wild are given in Table~\ref{tab:runtime}.

\begin{table}
\begin{center}
\begin{tabular}{|l|c|}
\hline
Stage & Time\\
\hline\hline
Optical flow & 336.41s\\
CNN evaluation & 1712.87s\\
Candidate set generation & 159.21s\\
Recombination & 10.86s\\
\hline
\end{tabular}
\end{center}
\caption{Run time of different stages of the pipeline when applied to all 30
frames of sequence 15 of Poses in the Wild. The experiment was performed using
two Intel Xeon E5-2620 processors and an NVIDIA K80 GPU.}
\label{tab:runtime}
\end{table}

\section{Discussion and future work}

Figure~\ref{fig:qualitative} shows that our algorithm significantly improves on
the results of \cyte{cherian2014mixing} for elbows and wrists, which are
typically the most difficult joints to detect. Further, it demonstrates that
recombination yields an appreciable performance increase over independent pose
estimation in each frame. This is particularly true for the wrists, which are
generally the fastest-moving joints in a video sequence.

Counterintuitively, the approach of Cherian \etal outperforms ours on shoulders
at low thresholds. This could be because of the size of joint--type
distribution (\ref{eqn:cnn-output}) learnt by the CNN; in the graphical model
used to generate pose candidate sets, the shoulder is attached to the upper arm,
upper torso and base of the neck, so if 13 types were learnt for each limb, then
there would be $13^3 = 2197$ different combinations of adjacent limb types for
each shoulder. Many of these combinations may not be well-represented in the
training set, which would hurt performance at test time.

It may be possible to address this by adopting a joint-based type
system~\cite{yang2011articulated} in place of a limb-based one. This would give
greater control over the effective number of joint types, which could allow for
the size of the joint--type distribution to be decreased significantly.
This could also yield an increase in performance, since the CNN evaluation time
for the final softmax layer in the network would decrease, as would the time
taken to copy and marginalise over the hundreds of thousands joint distribution
values produced by applying the fully convolutional network to large images.

Figure~\ref{fig:qualitative} illustrates a number of common failure modes.
Whilst our algorithm is robust to minor occlusions---where a hand lies slightly
outside a frame, for instance---of the kind shown in frames (a) and (d)--(f),
self-occlusion of subjects and partial occlusion of several joints have
proven more challenging, as in frames (b) and (f). Frames (a) and (c) also show
situations in which limb-like objects have confused the detector. Finally, rapid
limb motion was responsible for a large number of failures, including examples
(d)--(f).

\section{Conclusion}

This paper presented a two-stage approach to pose estimation in videos. In the
first stage, we generated a candidate pose set in each frame independently using
a graphical model incorporating CNN-derived features and type-based limb
deformation costs. In the second stage, we recombined the limbs associated with
each pose in each candidate set to produce a single best sequence of poses using
an approximate top-down approach to avoid the computational intractability
common to loopy graphical models. Evaluation on the challenging Poses in the
Wild data set showed that, for elbows and shoulders, our method significantly
improved on that of \cite{cherian2014mixing}. We closed with a discussion of the
accuracy and failure modes of the algorithm, and suggested that both accuracy
and performance could be improved by giving type labels to joints rather than
limbs.

\ifwacvfinal{%
\noindent\textbf{Acknowledgements}\quad
We would like to thank the authors of \cyte{chen2014articulated} and
\cyte{cherian2014mixing} for making their code publicly available.
}\fi

\ifwacvfinal\else\clearpage\fi
{\small
\bibliographystyle{latex-kit/ieee}
\bibliography{citations}
}
\end{document}
