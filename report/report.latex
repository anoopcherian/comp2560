\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{latex-kit/wacv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{cite}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
%\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

%{\wacvfinalcopy} % *** Uncomment this line for the final submission

\def\wacvPaperID{0000} % Fake paper ID for the quasi-conference
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ifwacvfinal\pagestyle{empty}\fi
\setcounter{page}{1}

% Macros which I find useful:
\renewcommand{\vec}{\mathbf}
\newcommand{\mat}{\mathbf}
\DeclareMathOperator{\nb}{nb}

\begin{document}

%%%%%%%%% TITLE
\title{Pose Estimation in Videos Using Convolutional Neural Networks and
Inter-Frame Recombination}

\author{Sam Toyer\\
The Australian National University\\
{\tt\small u5568237@anu.edu.au}
}

\maketitle
\ifwacvfinal\thispagestyle{empty}\fi

% Random notes about stuff go here.
%
% Figures which I can include:
% - Diagram showing entire pipeline
% - Correct detections
% - Incorrect detections
% - Illustration of recombination candidate set
% - PCK on PIW for elbows/wrists/shoulders, compared with the same papers which
%   Anoop compares with.
%
% Things I can put in the introduction (not all required; just looking through
% other papers to get ideas):
% - What pose estimation is and why it is important.
% - Challenges in pose estimation
% - Explain different classes of approaches to pose estimation (possibly
%   including their disadvantages). Just need to be careful not to tread on the
%   related work section.
% - Abstractly discuss the approach taken in the paper.
% - Summarise experiments performed and the results of those experiments.
%
% Things I can put in the related work section:
% - Talk about existing approaches and how they compare with one another in
%   terms of results and in terms of the computer vision tools used.
% - Explain why my approach is better (technically rather than empirically)
%
% Contents of the intra-frame part:
% - Talk about Yang & Ramanan's articulated model.
% - Discuss the type-based approach to part detection, and its advantages and
%   disadvantages.
% - Mention how the CNN works (esp. the fact that it's a fully convolutional
%   network, and also the way we produce a feature pyramid).
% - Graphical model and inference.
% - Candidate pose generation with NMS at each scale.
% - How do we use distance transforms to make GM inference more efficient? This
%   was something I didn't fully understand, so I might try to write about it in
%   the hope that I come to grok it better.
% - Talk about training.
%
% Contents of the inter-frame part:
% - Just explain how smoothing and recombination works, from optical flow down
%   to producing the final pose.
% - Should also explain whichever extensions I use from Anoop's paper.
% - How do we find our hyperparameters? This is hard, since we don't train
%   hyperparameters; really, we're just fiddling with them until they give us
%   something we want. I could say that I just started with Anoop's parameters,
%   then removed colour tracking and the skin histogram check, which really
%   shows that I have nothing up my sleeves. I could also mention that it's
%   theoretically possible to do a grid search for better parameters (maybe even
%   run such a search and show the results, with the disclaimer that we're
%   training on the test set).
%
% Things to consider putting in the conclusion and future work section (other
% than a summary, obviously):
% - Anoop's suggestion of using a CNN-based regressor to fine-tune joint
%   location estimates. Need to find some prior art on this; did Anoop just make
%   that up when I was talking to him? It seems like a useful insight, so
%   perhaps someone has tried it already.
% - Anoop's other suggestion about using an R-CNN-style approach to CNN forward
%   prop. Not totally convinced about the merits of that (you really only need
%   to identify windows in which there's definitely no human present at all,
%   which isn't what the R-CNN is doing with its semantic segmentation and
%   region joining steps), so I might leave it out.
% - Improve the CNN by using a newer model (e.g. ILSVRC '15 winners) or
%   decreasing the number of output layers from ~10,000 to a few hundred by
%   doing K-means on entire sets of outgoing limbs attached to a single joint.
%   Perhaps there's also a good PCA-based method hiding somewhere in there.
% - Diverse M-best for candidate generation, instead of the crap I'm using at
%   the moment.
% - Using LBP and optical flow to improve pairwise pose estimation.
%
% Remember the Heilmeier Catechism! Jochen said that most of these points should
% be addressed in the discussion:
%
% - What are you trying to do? Articulate your objectives using absolutely no
%   jargon.
% - How is it done today, and what are the limits of current practice?
% - What's new in your approach and why do you think it will be successful?
% - Who cares? If you're successful, what difference will it make?
%
% The Heilmeier Catechism also includes the following questions, although they
% are less relevant for our discussion:
%
% - What are the risks and the payoffs?
% - How much will it cost? How long will it take?
% - What are the midterm and final "exams" to check for success?

%%%%%%%%% ABSTRACT
\begin{abstract}
    This paper presents a method for estimation of articulated human poses in
    video frames. Our approach is comprised of a Convolutional Neural Network
    (CNN) and tree-structured graphical model for independent generation of
    candidate pose sets in each frame, followed by a recombination step which
    makes use of optical flow and pairwise deformation features to produce a
    single, consistent series of poses across an entire sequence. Evaluation on
    the Poses in the Wild dataset validates the usefulness of inter-frame pose
    recombination over single-frame pose estimation alone, and suggests that our
    method significantly improves upon the state-of-the-art in localising
    difficult body parts like wrists and elbows.
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}

The aim of human pose estimation is to take a sequence of static images or video
frames and output a series of two-dimensional ``skeletons'' representing the
locations of the limbs of any humans in those images. Pose estimation is useful
for higher-level computer vision tasks like identifying clothing
items~\cite{liu2012street,liu2012hi,yamaguchi2012parsing}, recognising
actions~\cite{yao2011does}, or even classifying objects which people interact
with~\cite{delaitre2012scene}.

\section{Related work}

\section{Single-frame candidate set generation}

Our method for generating a candidate pose set in each frame mostly follows that
of Chen and Yuille~\cite{chen2014articulated}, although we will describe the
relevant parts of their method in full.

\subsection{Model}

% TODO: Complete figure
\begin{figure}[t]
\begin{center}
\fbox{\rule{0pt}{1.5in} \rule{0.9\linewidth}{0pt}}
\end{center}
\caption{Illustration of the skeleton used to represent candidate poses within
each frame. This figure shows an 18-joint upper-body skeleton, as used for
evaluation in Section~\ref{sec:experiments}}
\label{fig:skeleton}
\end{figure}

Skeletons are represented by a graph consisting of a $\mathcal G = (\mathcal V,
\mathcal E)$ consisting of a set of vertices (also known as ``parts'' or
``joints'') $\mathcal V$ and a set of edges (commonly referred to as ``limbs'')
$\mathcal E \subseteq \mathcal V \times \mathcal V$, as illustrated in
Figure~\ref{fig:skeleton}. A complete pose $\vec{p} = (\vec l, \vec t)$ is
represented by a location $\vec{l_u}$ within an image $\mat I$ for each joint $u
\in \mathcal V$, and discrete ``types'' $t_{uv} \in \{1, \ldots, T_{uv}\}$ and
$t_{vu} \in \{1, \ldots, T_{vu}\}$ for each limb $(u, v) \in \mathcal E$.

Limb types, as popularised by Yang and Ramanan~\cite{yang2011articulated}, are
used to express the orientation and length of limbs. For example, long forearms
running from left to right might be assigned their own discrete type, as might
short, vertical forearms, medium-length diagonal forearms, and so on. During
inference, these types can be introduced as latent variables, in which case they
can be used to encourage the relative positions of body parts to take
anatomically reasonable values through the use of type-dependent limb
deformation costs.
% TODO: Clean this up
Further evidence for the type of a limb can be gleaned by inspecting small patches
of an image around the endpoints of that limb: for instance, an image of a
shoulder might give clues as to the direction in which the attached upper arm is
pointing. Both type-dependent deformation costs and IDPRs are discussed at
greater length in Section~\ref{sec:pairwise}.

Given a complete pose $(\vec l, \vec t)$ consisting of a vector part locations
$l$ and a vector of limb types $t$, and an image $\mat I$, the full score
$C(\mat l, \mat t \mid \mat I)$ of the pose can be decomposed into a sum of
unary costs and pairwise costs, like so:

\begin{equation}
\begin{split}
C(\vec l, \vec t \mid \mat I)
= &\sum_{u \in \mathcal V} \phi_u(\vec l_u \mid \mat I)\\
+ &\sum_{(u, v) \in \mathcal E}
    \psi_{uv}(\vec l_u, \vec l_v, t_{uv}, t_{vu} \mid \mat I)
\end{split}
\end{equation}

% TODO: This is obviously not a tree (since we have one type variable in each
% direction for each limb), so how are we doing inference? Are we just forcing
% t_{ij} = t_{ji}? I should read Chen & Yuille's code to understand this. Should
% also put a precise complexity on the recombination procedure. Oh, and an
% explanation of how distance transforms are used would be super handy!
Since $\mathcal G$ is a tree-structured graph, this cost can be minimised
efficiently using dynamic programming and distance transforms.

\subsection{Pairwise costs and IDPRs}
\label{sec:pairwise}

The pairwise cost $\phi_{uv}(\vec l_u, \vec l_v, t_{uv}, t_{vu} \mid \mat I)$
can itself be decomposed into the sum of type-dependent deformation costs and
IDPRs, as expressed by the following equation:

\begin{equation}
\begin{split}
\psi_{uv}(\vec l_u, \vec l_v, t_{uv}, t_{vu})
= &\vec w_{uvt_{uv}}^T d(\vec l_v - \vec l_u - \vec r_{uv t_{uv}})\\
+ &\vec w_{vut_{vu}}^T d(\vec l_u - \vec l_v - \vec r_{vu t_{vu}})\\
+ &w_{uv} \mathcal I(\vec l_u, t_{uv}, \mat I)\\
+ &w_{vu} \mathcal I(\vec l_v, t_{vu}, \mat I)
\end{split}
\end{equation}

Where $d(\vec v) = \begin{bmatrix}v_x^2 & v_y^2 & v_x & v_y\end{bmatrix}^T$ is a
deformation feature, and $\mathcal I$ gives an IDPR term, explained below.

Given a $K$-joint skeleton, we can define $J(\vec l, \mat I) \in \{0, \ldots,
K\}$ to indicate which of the $K$ joints is present at location $l$ in the image
$I$, with the special value of $J(\vec l, \mat I) = 0$ indicating that no joint
is present. Further, given that $J(\vec l, I) = u$, we can define $L(\vec l, u,
\mat I) \in \prod_{v \in \nb(u)} \{1, \ldots, T_{uv}\}$ to be a function
specifying the types of each limb connected to the joint $v$, where $\nb(u)$
denotes the neighbours of joint $u$ in the graph $\mathcal G$. Using this
notation, we can define $\mathcal I$ as:

\begin{equation}
\label{eqn:idpr}
\mathcal I(\vec l_u, t_{uv}, \mat I)
= \log p(L(\vec l_u, u, \mat I) = t_{uv} \mid J(\vec l_u, \mat I) = u)
\end{equation}

The inclusion of both $\mathcal I(\vec l_u, t_{uv}, \mat I)$ and $\mathcal
I(\vec l_v, t_{vu}, \mat I)$ ensures that the visual cues given by the parts at
either end of a limb can be used to infer the type of that limb.

\subsection{Unary costs}
\label{sec:unary}

The appearance term $\phi_u$ gives the log likelihood that a small, fixed-size
patch of the image centered at $\vec{l_u}$ contains the joint $u$:

\begin{equation}
\label{eqn:unary}
\phi_u(\vec l_u \mid \mat I)
= \log p(J(\vec l_u, \mat I) = u \mid \mat I)
\end{equation}

\subsection{Computing unaries and IDPR terms}
\label{sec:cnn}

To compute the unaries defined by Equation~\ref{eqn:unary} and the IDPRs defined
by Equation~\ref{eqn:idpr}, we train a CNN to predict the joint distribution
over $J$ and $L$ given $\mat I$:

\begin{equation}
\label{eqn:cnn-output}
% XXX: Yeah, this notation isn't as elegant as I expected. Chen & Yuille's isn't
% very elegant either, but now I see why.
p(J(\vec l, \mat I) = u, L(\vec l, u, I) = t_{uv} \mid \mat I)
\end{equation}

The unaries and IDPRs may be obtained from this distribution by marginalisation.

Our CNN architecture closely follows that of
AlexNet~\cite{krizhevsky2012imagenet}, and is identical to that
of~\cite{chen2014articulated}. At a high level, the network begins with a
convolution over the input, followed by two convolutional layers coupled with
max pooling and local response normalisation, after which three convolutional
layers are applied before two 4096-neuron fully-connected layers which feed into
the softmax output layer.

To minimise wasted computation, we convert the final fully connected layers of
the network to $1 \times 4096$ convolutions after the network has been trained.
This allows us to evaluate Equation~\ref{eqn:cnn-output} over a full-resolution
image in a single pass, rather than having to pass appropriately-sized crops
of the image through the network one at a time.

\subsection{Multi-scale detection}

\subsection{Learning}

\section{Inter-frame recombination}

% TODO: Complete figure
\begin{figure*}[t]
\begin{center}
\fbox{\rule{0pt}{1in} \rule{0.9\linewidth}{0pt}}
\end{center}
\caption{Diagram showing the recombination process. First, a fixed number of
candidate poses are generated in each frame, sorted descending by score. Next,
non-maximum suppression is performed on wrists and elbows to increase the
diversity of the top $N$ candidates. Finally, the top $N$ candidates in each
frame are fed to the recombination procedure, which greedily recombines limbs in
a top-down fashion whilst trying to minimise Equation~\ref{eqn:recomb-cost}.
% TODO: Reference actual equation
}
\label{fig:recombination}
\end{figure*}

Given a set of candidate poses in each frame, the approach of Cherian
\etal~\cite{cherian2014mixing} can be used to produce a consistent series of
poses across an entire video sequence. % TODO

\begin{equation}\label{eqn:recomb-cost}
% TODO
\end{equation}

\subsection{Use of optical flow}

\subsection{Choosing parameters}

\section{Experiments}
\label{sec:experiments}

% TODO: Complete figure
\begin{figure*}[t]
\begin{center}
\fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
\end{center}
\caption{Comparison of accurate and inaccurate pose estimates. The top row shows
several consecutive frames of a challenging sequence on which the model performs
well. The top row shows a number of individual frames in which the model
performed poorly.}
\label{fig:qualitative}
\end{figure*}

\subsection{Discussion}

\subsection{Future work}

\section{Conclusion}

% NOTE: Only include acknowledgments in the final copy (use \ifwacvstyle ... \fi
% to do this easily)!

\ifwacvfinal{%
\noindent\textbf{Acknowledgements}\quad
% XXX: I think that using [number] as a substitute for the paper name and year
% is considered poor style (was that what Steve complained about last semester?)
I would like to thank the authors of~\cite{chen2014articulated} and
\cite{cherian2014mixing} for making their code publicly available.
}\fi

{\small
\bibliographystyle{latex-kit/ieee}
\bibliography{citations}
}
\end{document}
