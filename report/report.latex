\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{latex-kit/wacv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{cite}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
%\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

%{\wacvfinalcopy} % *** Uncomment this line for the final submission

\def\wacvPaperID{0000} % Fake paper ID for the quasi-conference
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ifwacvfinal\pagestyle{empty}\fi
\setcounter{page}{1}
\begin{document}

%%%%%%%%% TITLE
\title{Pose Estimation in Videos Using Convolutional Neural Networks and
Inter-Frame Recombination}

\author{Sam Toyer\\
The Australian National University\\
{\tt\small u5568237@anu.edu.au}
}

\maketitle
\ifwacvfinal\thispagestyle{empty}\fi

% Random notes about stuff go here.
%
% Figures which I can include:
% - Diagram showing entire pipeline
% - Correct detections
% - Incorrect detections
% - Illustration of recombination candidate set
% - PCK on PIW for elbows/wrists/shoulders, compared with the same papers which
%   Anoop compares with.
%
% Things I can put in the introduction (not all required; just looking through
% other papers to get ideas):
% - What pose estimation is and why it is important.
% - Challenges in pose estimation
% - Explain different classes of approaches to pose estimation (possibly
%   including their disadvantages). Just need to be careful not to tread on the
%   related work section.
% - Abstractly discuss the approach taken in the paper.
% - Summarise experiments performed and the results of those experiments.
%
% Things I can put in the related work section:
% - Talk about existing approaches and how they compare with one another in
%   terms of results and in terms of the computer vision tools used.
% - Explain why my approach is better (technically rather than empirically)
%
% Contents of the intra-frame part:
% - Talk about Yang & Ramanan's articulated model.
% - Discuss the type-based approach to part detection, and its advantages and
%   disadvantages.
% - Mention how the CNN works (esp. the fact that it's a fully convolutional
%   network, and also the way we produce a feature pyramid).
% - Graphical model and inference.
% - Candidate pose generation with NMS at each scale.
% - How do we use distance transforms to make GM inference more efficient? This
%   was something I didn't fully understand, so I might try to write about it in
%   the hope that I come to grok it better.
% - Talk about training.
%
% Contents of the inter-frame part:
% - Just explain how smoothing and recombination works, from optical flow down
%   to producing the final pose.
% - Should also explain whichever extensions I use from Anoop's paper.
% - How do we find our hyperparameters? This is hard, since we don't train
%   hyperparameters; really, we're just fiddling with them until they give us
%   something we want. I could say that I just started with Anoop's parameters,
%   then removed colour tracking and the skin histogram check, which really
%   shows that I have nothing up my sleeves. I could also mention that it's
%   theoretically possible to do a grid search for better parameters (maybe even
%   run such a search and show the results, with the disclaimer that we're
%   training on the test set).
%
% Things to consider putting in the conclusion and future work section (other
% than a summary, obviously):
% - Anoop's suggestion of using a CNN-based regressor to fine-tune joint
%   location estimates. Need to find some prior art on this; did Anoop just make
%   that up when I was talking to him? It seems like a useful insight, so
%   perhaps someone has tried it already.
% - Anoop's other suggestion about using an R-CNN-style approach to CNN forward
%   prop. Not totally convinced about the merits of that (you really only need
%   to identify windows in which there's definitely no human present at all,
%   which isn't what the R-CNN is doing with its semantic segmentation and
%   region joining steps), so I might leave it out.
% - Improve the CNN by using a newer model (e.g. ILSVRC '15 winners) or
%   decreasing the number of output layers from ~10,000 to a few hundred by
%   doing K-means on entire sets of outgoing limbs attached to a single joint.
%   Perhaps there's also a good PCA-based method hiding somewhere in there.
% - Diverse M-best for candidate generation, instead of the crap I'm using at
%   the moment.
% - Using LBP and optical flow to improve pairwise pose estimation.

% Macros which I find useful:
\renewcommand{\vec}{\mathbf}
\newcommand{\mat}{\mathbf}

%%%%%%%%% ABSTRACT
\begin{abstract}
    This paper presents a method for estimation of articulated human poses in
    video frames. Our approach is comprised of a Convolutional Neural Network
    (CNN) and tree-structured graphical model for independent generation of
    candidate pose sets in each frame, followed by a recombination step which
    makes use of optical flow and pairwise deformation features to produce a
    single, consistent series of poses across an entire sequence. Evaluation on
    the Poses in the Wild dataset validates the usefulness of inter-frame pose
    recombination over single-frame pose estimation alone, and suggests that our
    method significantly improves upon the state-of-the-art in localising
    difficult body parts like wrists and elbows.
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}

The aim of human pose estimation is to take a sequence of static images or video
frames and output a series of two-dimensional ``skeletons'' representing the
locations of the limbs of any humans in those images. Pose estimation is useful
for higher-level computer vision tasks like identifying clothing
items~\cite{liu2012street,liu2012hi,yamaguchi2012parsing}, recognising
actions~\cite{yao2011does}, or even classifying objects which people interact
with~\cite{delaitre2012scene}.

\section{Related work}

\section{Single-frame candidate set generation}

Our method for generating a candidate pose set in each frame mostly follows that
of Chen and Yuille~\cite{chen2014articulated}, although we will describe the
relevant parts of their method in full.

\subsection{Model}

% TODO: Complete figure
\begin{figure}[t]
\begin{center}
\fbox{\rule{0pt}{1.5in} \rule{0.9\linewidth}{0pt}}
\end{center}
\caption{Illustration of the skeleton used to represent candidate poses within
each frame. This figure shows an 18-joint upper-body skeleton, as used for
evaluation in Section~\ref{sec:experiments}}
\label{fig:skeleton}
\end{figure}

Skeletons are represented by a graph consisting of a $\mathcal G = (\mathcal V,
\mathcal E)$ consisting of a set of vertices (also known as ``parts'' or
``joints'') $\mathcal V$ and a set of edges (commonly referred to as ``limbs'')
$\mathcal E \subseteq \mathcal V \times \mathcal V$, as illustrated in
Figure~\ref{fig:skeleton}. A complete pose $\vec{p} = (\vec l, \vec t)$ is
represented by a location $\vec{l_u}$ within an image $\mat I$ for each joint $u
\in \mathcal V$, and discrete ``types'' $t_{uv}$ and $t_{vu}$ for each limb $(u,
v) \in \mathcal E$.

Limb types, as popularised by Yang and Ramanan~\cite{yang2011articulated}, are
used to express the orientation and length of limbs. For example, long forearms
running from left to right might be assigned their own discrete type, as might
short, vertical forearms, medium-length diagonal forearms, and so on. During
inference, these types can be introduced as latent variables, in which case they
can be used to encourage the relative positions of body parts to take
anatomically reasonable values through the use of type-dependent limb
deformation costs.
% TODO: Clean this up
Further evidence for the type of a limb can be gleaned by inspecting small patches
of an image around the endpoints of that limb: for instance, an image of a
shoulder might give clues as to the direction in which the attached upper arm is
pointing. Both type-dependent deformation costs and IDPRs are discussed at
greater length in Section~\ref{sec:pairwise}.

Given a pose $\vec p = (\vec l, \vec t)$, and an image $\mat I$, the full score
$C(\mat l, \mat t \mid \mat I)$ can be decomposed into a sum of unary costs and
pairwise costs, like so:

\begin{equation}
\begin{split}
C(\vec l, \vec t \mid \mat I)
= &\sum_{u \in \mathcal V} \phi_u(\vec l_u \mid \mat I)\\
+ &\sum_{(u, v) \in \mathcal E}
    \psi_{uv}(\vec l_u, \vec l_v, t_{uv}, t_{vu} \mid \mat I)
\end{split}
\end{equation}

Since $\mathcal G$ is a tree-structured graph, this cost can be minimised
efficiently using dynamic programming and distance transforms.

\subsection{Pairwise costs and IDPRs}
\label{sec:pairwise}

The pairwise cost $\phi_{uv}(\vec l_u, \vec l_v, t_{uv}, t_{vu} \mid \mat I)$
can itself be decomposed into the sum of type-dependent deformation costs and
Image-Dependent Pairwise Relations (IDPRs), as expressed by the following
equation:

\begin{equation}
\begin{split}
\psi_{uv}(\vec l_u, \vec l_v, t_{uv}, t_{vu})
= &\vec w_{uvt_{uv}}^T d(\vec l_v - \vec l_u - \vec r_{uv t_{uv}})\\
+ &\vec w_{vut_{vu}}^T d(\vec l_u - \vec l_v - \vec r_{vu t_{vu}})\\
+ &w_{uv} \log p() % TODO
\end{split}
\end{equation}

Where $d(\vec v) = \begin{bmatrix}v_x^2 & v_y^2 & v_x & v_y\end{bmatrix}^T$ is a
deformation feature.

\subsection{Unary costs}
\label{sec:unary}

% TODO
The appearance term $\phi(\vec l_u \mid I) = \log(p(c = i) \mid \mat I)$

\subsection{Learning}

\section{Inter-frame recombination}

% TODO: Complete figure
\begin{figure*}[t]
\begin{center}
\fbox{\rule{0pt}{1in} \rule{0.9\linewidth}{0pt}}
\end{center}
\caption{Diagram showing the recombination process. First, a fixed number of
candidates are generated in each frame, % TODO
}
\label{fig:recombination}
\end{figure*}

Given a set of candidate poses in each frame, the approach of Cherian
\etal~\cite{cherian2014mixing} can be used to produce a consistent series of
poses across an entire video sequence. % TODO

\subsection{Use of optical flow}

\subsection{Choosing parameters}

\section{Experiments}
\label{sec:experiments}

% TODO: Complete figure
\begin{figure*}[t]
\begin{center}
\fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
\end{center}
\caption{Comparison of accurate and inaccurate pose estimates. The top row shows
several consecutive frames of a challenging sequence on which the model performs
well. The top row shows a number of individual frames in which the model
performed poorly.}
\label{fig:recombination}
\end{figure*}

\subsection{Discussion}

\section{Conclusion and future work}

% NOTE: Only include acknowledgments in the final copy (use \ifwacvstyle ... \fi
% to do this easily)!

\ifwacvfinal{%
\noindent\textbf{Acknowledgements}\quad
% XXX: I think that using [number] as a substitute for the paper name and year
% is considered poor style (was that what Steve complained about last semester?)
I would like to thank the authors of~\cite{chen2014articulated} and
\cite{cherian2014mixing} for making their code publicly available.
}\fi

{\small
\bibliographystyle{latex-kit/ieee}
\bibliography{citations}
}
\end{document}
